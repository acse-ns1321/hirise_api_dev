{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOon9aBh3FoZF9xK7/VNOPS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acse-ns1321/hirise_api_dev/blob/main/Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CuAIPe2Xdxb8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt # plotting library\n",
        "import numpy as np # this module is useful to work with numerical arrays\n",
        "import pandas as pd \n",
        "import random \n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader,random_split\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import cv2\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset,Dataset, DataLoader,random_split\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from glob import glob\n",
        "import cv2\n",
        "import os,sys\n",
        "os.environ['OPEgrid_columnsV_IO_ENABLE_JASPER'] = 'true'\n",
        "import numpy as np\n",
        "from PIL import Image, ImageFile, ImageOps\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "import math\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "Image.MAX_IMAGE_PIXELS = None\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "# from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import torch packages that help us define our network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms.transforms import Normalize\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "# Package that allows us to summarize our network\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# dir_path = os.path.dirname(os.path.realpath(__file__))\n",
        "# parent_dir_path = os.path.abspath(os.path.join(dir_path, os.pardir))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr7V86XKjK08",
        "outputId": "dcb53f98-1165-406c-8950-100c3bf6665c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Hirise_Image_Dataset(Dataset):\n",
        "    \"\"\"Hirise Image dataset.\"\"\"\n",
        "    def __init__(self,\n",
        "                 path_to_images,\n",
        "                 transform=None):\n",
        "        # ------------------------------------------------------------------------------\n",
        "        # path_to_images: where you put the image dataset\n",
        "        # transform:  data transform\n",
        "        # img_size: resize all images to a standard size\n",
        "        # ------------------------------------------------------------------------------\n",
        "\n",
        "        # Load all the images and their labels\n",
        "        self.dataset = datasets.ImageFolder(path_to_images, transform=transform)\n",
        "        self.len = len(self.dataset.imgs)\n",
        "        self.path_to_images = path_to_images\n",
        "\n",
        "        # ------------------------------------------------------------------------------\n",
        "        # Split the data into train and test data 80 : 20\n",
        "        # ------------------------------------------------------------------------------\n",
        "        # Calculate the lengths of the vectors\n",
        "        lengths = [int(np.ceil(len(self.dataset)*0.8)), int(np.floor(len(self.dataset)*0.2))]\n",
        "\n",
        "\n",
        "        # Extract the images and labels   \n",
        "        self.train_dataset, self.test_dataset = random_split(self.dataset, lengths)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of samples\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "I-1eEjXNlJUu"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Data_Preparation:\n",
        "    \"\"\"Class that allows for data prepartion as part of the preprocessing of the hirise images. \"\"\"\n",
        "    def remove_background(self,file_name):\n",
        "        src = cv2.imread(file_name, 1)\n",
        "        tmp = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n",
        "        _,alpha = cv2.threshold(tmp,0,255,cv2.THRESH_BINARY)\n",
        "        b, g, r = cv2.split(src)\n",
        "        rgba = [b,g,r, alpha]\n",
        "        dst = cv2.merge(rgba,4)\n",
        "        if np.allclose(np.asarray(dst), 0):\n",
        "            os.remove(file_name)\n",
        "        else:\n",
        "            cv2.imwrite(file_name, dst)\n",
        "\n",
        "    def resize_image(self, folder_path, resized_images_folder_path, pixel_length_cm = 250):\n",
        "        reduce_factor = 25/pixel_length_cm\n",
        "        imgfiles = glob(f\"{folder_path}/*.IMG\")\n",
        "\n",
        "        # Convert to PIL Imgae\n",
        "        img_list = []\n",
        "        for img in tqdm(imgfiles):\n",
        "            img_list.append(Image.open(img))\n",
        "\n",
        "        if os.path.isdir(resized_images_folder_path):\n",
        "            os.chdir(resized_images_folder_path)\n",
        "        else:\n",
        "            os.makedirs(resized_images_folder_path)\n",
        "            os.chdir(resized_images_folder_path)\n",
        "\n",
        "        for im,name in tqdm(zip(img_list,imgfiles)):         \n",
        "            resized_im = im.resize((round(im.size[0]*reduce_factor), round(im.size[1]*reduce_factor)))\n",
        "            print(\"Hello\")\n",
        "            try:\n",
        "              print(name)\n",
        "              resized_im.save(name.split('/')[-1]+'_resizedimage.jpg')\n",
        "            except:\n",
        "              pass\n",
        "            \n",
        "    def tile_images(self, folder_path,image_directory, image_size_pixels, resized = True,remove_background = True):\n",
        "        if resized:\n",
        "            imgfiles = glob(f\"{folder_path}/*.jpg\")\n",
        "        else:\n",
        "            imgfiles = glob(f\"{folder_path}/*.IMG\")\n",
        "        # Convert to PIL Imgae\n",
        "        img_list = []\n",
        "        for img in imgfiles:\n",
        "            img_list.append(Image.open(img))\n",
        "\n",
        "        if os.path.isdir(image_directory):\n",
        "            os.chdir(image_directory)\n",
        "        else:\n",
        "            os.makedirs(image_directory)\n",
        "            os.chdir(image_directory)\n",
        "\n",
        "        for img,name in tqdm(zip(img_list,imgfiles)):\n",
        "            try:\n",
        "                im = np.asarray(img)\n",
        "                for r in range(0,math.ceil(im.shape[0]),image_size_pixels):\n",
        "                    for c in range(0,math.ceil(im.shape[1]),image_size_pixels):\n",
        "                            f_name = name.split('/')[-1].split('.')[0] + f\"_{r}_{c}.jpg\"\n",
        "                            cv2.imwrite(str(f_name), im[r:r+image_size_pixels, c:c+image_size_pixels,:] )\n",
        "                            if remove_background:\n",
        "                                Data_Preparation.remove_background(self,file_name =f_name)\n",
        "            except:\n",
        "                pass                \n",
        "\n",
        "        # sys.path.insert(0, parent_dir_path)\n",
        "\n",
        "    def convert_to_grayscale(self, folder_path,image_directory, remove_background = True):\n",
        "        imgfiles = glob(f\"{folder_path}/*.jpg\")\n",
        "\n",
        "        im_list = []\n",
        "        # Convert to PIL Imgae\n",
        "        for img in imgfiles:\n",
        "            im_list.append(cv2.imread(img, 1))\n",
        "        \n",
        "        if os.path.isdir(image_directory):\n",
        "            os.chdir(image_directory)\n",
        "        else:\n",
        "            os.makedirs(image_directory)\n",
        "            os.chdir(image_directory)\n",
        "\n",
        "        for img,name in zip(im_list,imgfiles):\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            f_name = \"gray_\" + name.split('\\\\')[1]\n",
        "            try:\n",
        "                cv2.imwrite(f_name, gray)\n",
        "            except:\n",
        "                pass\n",
        "            if remove_background:\n",
        "                Data_Preparation.remove_background(self,file_name =f_name)\n",
        "        # sys.path.insert(0, parent_dir_path)\n",
        "\n",
        "    def remove_image_with_empty_pixels(self, folder_path, max_percentage_empty_space = 20):\n",
        "        imgfiles = glob(f\"{folder_path}/*.jpg\")\n",
        "\n",
        "        if os.path.isdir(folder_path):\n",
        "            os.chdir(folder_path)\n",
        "        else:\n",
        "            print(\"ERROR!!\")\n",
        "\n",
        "        for f_name in tqdm(imgfiles):\n",
        "            empty = 0\n",
        "            img = Image.open(f_name.split('\\\\')[1])\n",
        "            width, height = img.width, img.height\n",
        "            total = width * height\n",
        "            for pixel in img.getdata():\n",
        "                if pixel == (0,0,0,0) or pixel == (0,0,0):            \n",
        "                    empty += 1\n",
        "            percent = round((empty * 100.0/total),1)\n",
        "            if(percent >= max_percentage_empty_space):            \n",
        "                os.remove(f_name.split('\\\\')[1])\n",
        "        # sys.path.insert(0, parent_dir_path)\n",
        "\n",
        "    def get_image_dataset(self,f_path,  transform_data =  None ):\n",
        "        if not transform_data:\n",
        "            transform_data = transforms.Compose([transforms.ToTensor()])\n",
        "        # transform_data= transforms.Compose([transforms.ToTensor(), transforms.Grayscale(num_output_channels=1)])\n",
        "        dataset = Hirise_Image_Dataset(path_to_images = f_path, transform = transform_data)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def get_train_test_val_tensors(self, dataset):\n",
        "            m=len(dataset.train_dataset)\n",
        "\n",
        "            train_ds, val_ds = random_split(dataset.train_dataset, [math.floor(m-m*0.2), math.ceil(m*0.2)])\n",
        "            # ------------------Training Data ----------------------------------------------\n",
        "            # Empty lists to store the training data\n",
        "            train_list = []\n",
        "            # Append from the MedicalMNIST Object the training target and labels\n",
        "            for data in train_ds:\n",
        "                train_list.append(data[0])\n",
        "\n",
        "            train_tensor = torch.Tensor(len(train_list))\n",
        "            try :\n",
        "                torch.cat(train_list,out = train_tensor)\n",
        "            except :\n",
        "                pass\n",
        "            # ------------------- --- Test Data ---------------------------------------------\n",
        "            # Empty lists to store the test data\n",
        "            test_list = []\n",
        "            for data in dataset.test_dataset:\n",
        "                test_list.append(data[0])\n",
        "\n",
        "            test_tensor = torch.Tensor(len(test_list))\n",
        "            try:\n",
        "                torch.cat(test_list,out = test_tensor)\n",
        "            except :\n",
        "                pass\n",
        "            # ------------------- --- Val Data ---------------------------------------------\n",
        "            # Empty lists to store the test data\n",
        "            val_list = []\n",
        "            for data in val_ds:\n",
        "                val_list.append(data[0])\n",
        "\n",
        "            val_tensor = torch.Tensor(len(val_list))\n",
        "\n",
        "            try:\n",
        "                torch.cat(val_list,out = val_tensor)\n",
        "            except :\n",
        "                pass\n",
        "            return  train_tensor, test_tensor, val_tensor\n",
        "\n",
        "    def get_train_test_val_dataloader(self, train_data, test_data, val_data,  b_size = 128):\n",
        "        # Create TorchTensor Datasets containing training_data, testing_data, validation_data\n",
        "        training_data = TensorDataset(train_data,train_data.long() )\n",
        "        validation_data = TensorDataset(val_data,val_data.long() )\n",
        "        testing_data = TensorDataset(test_data, test_data.long())\n",
        "        train_loader = DataLoader(dataset = training_data, batch_size=b_size)\n",
        "        valid_loader = DataLoader(dataset = validation_data, batch_size=b_size)\n",
        "        test_loader = DataLoader(dataset = testing_data, batch_size=b_size,shuffle=True)\n",
        "        return train_loader,  test_loader, valid_loader\n",
        "\n",
        "    def show_training_data(self, dataset, grid_rows=5, grid_columns=5):\n",
        "        \"\"\" Prints the traning data in a grid\"\"\"\n",
        "        # Set up axes and subplots\n",
        "        fig, axarr = plt.subplots(grid_rows, grid_columns, figsize=(10, 10))\n",
        "\n",
        "        # Loops to run over the grid\n",
        "        for i in range(grid_rows):\n",
        "            for j in range(grid_columns):\n",
        "\n",
        "                # Generate a random index in the training dataset\n",
        "                idx = random.randint(0, len(dataset.train_dataset))\n",
        "\n",
        "                # Get the sample and target fromthe traiig dataset\n",
        "                sample, _ = dataset.train_dataset[idx]\n",
        "\n",
        "                try:\n",
        "                    # Exception handling - if it is PIL\n",
        "                    axarr[i][j].imshow(sample, cmap = \"gray\")\n",
        "                except:\n",
        "                    # If tensor of shape CHW\n",
        "                    axarr[i][j].imshow(sample.permute(1,2,0), cmap = \"gray\") \n",
        "                # # Get the classes of the target data\n",
        "                # target_name = dataset.dataset.targets[target]\n",
        "                # # Label each image eith the target name and the class it belongs to\n",
        "                # axarr[i][j].set_title(\"%s (%i)\"%(target_name, target))\n",
        "        # Deine the grid layout and padding\n",
        "        \n",
        "        fig.tight_layout(pad=1)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "7nrbn76mihjU"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class CAE_Encoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, encoded_space_dim,fc2_input_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        ### Convolutional section\n",
        "        self.encoder_cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 256, 3, stride=2, padding=0),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        ### Flatten layer\n",
        "        self.flatten = nn.Flatten(start_dim=1)\n",
        "        ### Linear section\n",
        "        self.encoder_lin = nn.Sequential(\n",
        "            nn.Linear(1 * 3 * 32, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, encoded_space_dim)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.encoder_cnn(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.encoder_lin(x)\n",
        "        return x\n",
        "\n",
        "class CAE_Decoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, encoded_space_dim,fc2_input_dim):\n",
        "        super().__init__()\n",
        "        self.decoder_lin = nn.Sequential(\n",
        "            nn.Linear(encoded_space_dim, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 1 * 3 * 32),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.unflatten = nn.Unflatten(dim=1, \n",
        "        unflattened_size=(256, 1, 3))\n",
        "\n",
        "        self.decoder_conv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 16, 3, \n",
        "            stride=2, output_padding=0),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 8, 3, stride=2, \n",
        "            padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(8, 1, 3, stride=2, \n",
        "            padding=1, output_padding=1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.decoder_lin(x)\n",
        "        x = self.unflatten(x)\n",
        "        x = self.decoder_conv(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = CAE_Encoder()\n",
        "        self.decoder = CAE_Decoder()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        latent = self.encoder(x)\n",
        "        x_recon = self.decoder(latent)\n",
        "        return x_recon\n",
        "\n",
        "latent_dims = 10\n",
        "num_epochs = 50\n",
        "batch_size = 128\n",
        "capacity = 64\n",
        "learning_rate = 1e-3\n",
        "use_gpu = True\n",
        "\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "### Define an optimizer (both for the encoder and the decoder!)\n",
        "lr= 0.001\n",
        "\n",
        "### Set the random seed for reproducible results\n",
        "torch.manual_seed(0)\n",
        "\n",
        "### Initialize the two networks\n",
        "d = 4\n",
        "\n",
        "#model = Autoencoder(encoded_space_dim=encoded_space_dim)\n",
        "encoder = CAE_Encoder(encoded_space_dim=d,fc2_input_dim=256)\n",
        "decoder = CAE_Decoder(encoded_space_dim=d,fc2_input_dim=256)\n",
        "params_to_optimize = [\n",
        "    {'params': encoder.parameters()},\n",
        "    {'params': decoder.parameters()}\n",
        "]\n",
        "\n",
        "optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n",
        "\n",
        "# Check if the GPU is available\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f'Selected device: {device}')\n",
        "\n",
        "# Move both the encoder and the decoder to the selected device\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "\n",
        "class Training:\n",
        "    ### Training function\n",
        "    def train_CAE(encoder, decoder, device, dataloader, loss_fn, optimizer):\n",
        "        # Set train mode for both the encoder and the decoder\n",
        "        encoder.train()\n",
        "        decoder.train()\n",
        "        train_loss = []\n",
        "        # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
        "        for image_batch, _ in dataloader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
        "            # Move tensor to the proper device\n",
        "            image_batch = image_batch.to(device)\n",
        "            # Encode data\n",
        "            encoded_data = encoder(image_batch)\n",
        "            # Decode data\n",
        "            decoded_data = decoder(encoded_data)\n",
        "            # Evaluate loss\n",
        "            loss = loss_fn(decoded_data, image_batch)\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Print batch loss\n",
        "            print('\\t partial train loss (single batch): %f' % (loss.data))\n",
        "            train_loss.append(loss.detach().cpu().numpy())\n",
        "\n",
        "        return np.mean(train_loss)\n",
        "      ### Training function\n",
        "    def train_epoch(encoder, decoder, device, dataloader, loss_fn, optimizer):\n",
        "                # Set train mode for both the encoder and the decoder\n",
        "        encoder.train()\n",
        "        decoder.train()\n",
        "        train_loss = []\n",
        "        # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
        "        for image_batch in dataloader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
        "            # Move tensor to the proper device\n",
        "            # print(image_batch)\n",
        "            # image_batch= np.asarray(image_batch)\n",
        "            # image_batch = sum(image_batch)\n",
        "            # print(image_batch.permute(128, 1, 256, 256))\n",
        "            image_batch = image_batch.to(device)\n",
        "            # Encode data\n",
        "            encoded_data = encoder(image_batch)\n",
        "            # Decode data\n",
        "            decoded_data = decoder(encoded_data.squeeze(128, 256, 256))\n",
        "            # Evaluate loss\n",
        "            loss = loss_fn(decoded_data, image_batch)\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Print batch loss\n",
        "            print('\\t partial train loss (single batch): %f' % (loss.data))\n",
        "            train_loss.append(loss.detach().cpu().numpy())\n",
        "\n",
        "        return np.mean(train_loss)\n",
        "\n",
        "class Testing:\n",
        "    \n",
        "    ### Testing function\n",
        "    def test_epoch(encoder, decoder, device, dataloader, loss_fn):\n",
        "        # Set evaluation mode for encoder and decoder\n",
        "        encoder.eval()\n",
        "        decoder.eval()\n",
        "        with torch.no_grad(): # No need to track the gradients\n",
        "            # Define the lists to store the outputs for each batch\n",
        "            conc_out = []\n",
        "            conc_label = []\n",
        "            for image_batch in dataloader:\n",
        "                # Move tensor to the proper device\n",
        "                image_batch = image_batch.to(device)\n",
        "                # Encode data\n",
        "                encoded_data = encoder(image_batch)\n",
        "                # Decode data\n",
        "                decoded_data = decoder(encoded_data)\n",
        "                # Append the network output and the original image to the lists\n",
        "                conc_out.append(decoded_data.cpu())\n",
        "                conc_label.append(image_batch.cpu())\n",
        "            # Create a single tensor with all the values in the lists\n",
        "            conc_out = torch.cat(conc_out)\n",
        "            conc_label = torch.cat(conc_label) \n",
        "            # Evaluate global loss\n",
        "            val_loss = loss_fn(conc_out, conc_label)\n",
        "        return val_loss.dat\n",
        "  \n",
        "\n",
        "\n",
        "class Plot_losses:\n",
        "    def plot_ae_outputs(encoder,decoder,n=10):\n",
        "        plt.figure(figsize=(16,4.5))\n",
        "        targets = Train_Model.tst.targets.numpy()\n",
        "        t_idx = {i:np.where(targets==i)[0][0] for i in range(n)}\n",
        "        for i in range(n):\n",
        "            ax = plt.subplot(2,n,i+1)\n",
        "            img = Train_Model.tst[t_idx[i]][0].unsqueeze(0).to(device)\n",
        "            encoder.eval()\n",
        "            decoder.eval()\n",
        "            with torch.no_grad():\n",
        "                rec_img  = decoder(encoder(img))\n",
        "            plt.imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
        "            ax.get_xaxis().set_visible(False)\n",
        "            ax.get_yaxis().set_visible(False)  \n",
        "            if i == n//2:\n",
        "                ax.set_title('Original images')\n",
        "            ax = plt.subplot(2, n, i + 1 + n)\n",
        "            plt.imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')  \n",
        "            ax.get_xaxis().set_visible(False)\n",
        "            ax.get_yaxis().set_visible(False)  \n",
        "            if i == n//2:\n",
        "                ax.set_title('Reconstructed images')\n",
        "            plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue3tZYcYe1pN",
        "outputId": "e87ec368-6a78-4536-f2b4-807799eb75be"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Train_Model(folder_path):\n",
        "    num_epochs = 30\n",
        "    diz_loss = {'train_loss':[],'val_loss':[]}\n",
        "    transform1= transforms.Compose([transforms.ToTensor(), transforms.Grayscale(num_output_channels=1)])\n",
        "    dp = Data_Preparation()\n",
        "    dataset1 = dp.get_image_dataset(f_path = folder_path, transform_data = transform1)\n",
        "    tr,tst,val = dp.get_train_test_val_tensors(dataset = dataset1)\n",
        "    train_loader,test_loader, val_l = dp.get_train_test_val_dataloader(tr,tst,val )\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss =Training.train_epoch(encoder,decoder,device,\n",
        "        train_loader,loss_fn,optim)\n",
        "        val_loss = Testing.test_epoch(encoder,decoder,device,test_loader,loss_fn)\n",
        "        print('\\n EPOCH {}/{} \\t train loss {} \\t val loss {}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
        "        diz_loss['train_loss'].append(train_loss)\n",
        "        diz_loss['val_loss'].append(val_loss)\n",
        "        Plot_losses.plot_ae_outputs(encoder,decoder,n=10)\n"
      ],
      "metadata": {
        "id": "7ThfWASNiQm-"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dp = Data_Preparation()\n"
      ],
      "metadata": {
        "id": "ZYOp6tTWjueG"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dp.resize_image(folder_path='/content/drive/MyDrive/data-download',resized_images_folder_path= '/content/drive/MyDrive/data-download/resized-images/',pixel_length_cm = 250)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXeRcNWqkWi-",
        "outputId": "e3ac8ec5-0769-460c-e52e-cea3d0fbe5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:00<00:00, 86.99it/s]\n",
            "1it [03:54, 234.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_026031_2295_UNFILTERED_COLOR.IMG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r2it [05:23, 148.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_030429_2160_UNFILTERED_COLOR.IMG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r3it [09:14, 186.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_025952_2250_UNFILTERED_COLOR.IMG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r4it [09:54, 128.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_028953_1830_UNFILTERED_COLOR.IMG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r5it [13:41, 164.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_032100_1645_UNFILTERED_COLOR.IMG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r6it [14:28, 124.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_020887_1670_COLOR.IMG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r7it [16:36, 125.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_027173_1635_UNFILTERED_COLOR.IMG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r8it [18:04, 113.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_020115_0985_COLOR.IMG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r9it [19:42, 108.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_046350_2310_UNFILTERED_COLOR.IMG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r10it [21:04, 100.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_040829_1590_UNFILTERED_COLOR.IMG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r11it [22:10, 89.92s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_045314_1985_UNFILTERED_COLOR.IMG\n",
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_046329_1725_UNFILTERED_COLOR.IMG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r12it [22:23, 66.51s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dp = Data_Preparation()\n",
        "# dp.tile_images(folder_path='/content/drive/MyDrive/data-download/resized-images/',image_directory  ='/content/drive/MyDrive/data-download/tiled-images/' , image_size_pixels = 256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy3eC_BUvVxI",
        "outputId": "c44fbd04-a9a9-4cf2-f85c-90fbb786307c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11it [00:09,  1.14it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !zip -r tiled_images.zip /content/drive/MyDrive/data-download/tiled-images/ "
      ],
      "metadata": {
        "id": "TGT-6xQQjAyb"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/MyDrive/Images/img/'\n",
        "num_epochs = 30\n",
        "diz_loss = {'train_loss':[],'val_loss':[]}\n",
        "transform1= transforms.Compose([transforms.ToTensor(), transforms.Grayscale(num_output_channels=1)])\n",
        "dp = Data_Preparation()\n",
        "dataset1 = dp.get_image_dataset(f_path = folder_path, transform_data = transform1)\n",
        "tr,tst,val = dp.get_train_test_val_tensors(dataset = dataset1)\n",
        "train_loader,test_loader, val_l = dp.get_train_test_val_dataloader(tr,tst,val)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss =Training.train_epoch(encoder,decoder,device, train_loader,loss_fn,optim)\n",
        "    val_loss = Testing.test_epoch(encoder,decoder,device,test_loader,loss_fn)\n",
        "    print('\\n EPOCH {}/{} \\t train loss {} \\t val loss {}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
        "    diz_loss['train_loss'].append(train_loss)\n",
        "    diz_loss['val_loss'].append(val_loss)\n",
        "    Plot_losses.plot_ae_outputs(encoder,decoder,n=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Mor_eObt0ohM",
        "outputId": "8a1bb38d-3367-4e0f-d5cf-38065e58eb6c"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-161-286d07aeea5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mTraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n EPOCH {}/{} \\t train loss {} \\t val loss {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-158-0e49854b85dd>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(encoder, decoder, device, dataloader, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;31m# image_batch = sum(image_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;31m# print(image_batch.permute(128, 1, 256, 256))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mimage_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0;31m# Encode data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mencoded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PoYZDG0d9AeT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}