{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM3o3hX1cR/B1+ubfGtpbGm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acse-ns1321/hirise_api_dev/blob/main/Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CuAIPe2Xdxb8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt # plotting library\n",
        "import numpy as np # this module is useful to work with numerical arrays\n",
        "import pandas as pd \n",
        "import random \n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader,random_split\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import cv2\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset,Dataset, DataLoader,random_split\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from glob import glob\n",
        "import cv2\n",
        "import os,sys\n",
        "os.environ['OPEgrid_columnsV_IO_ENABLE_JASPER'] = 'true'\n",
        "import numpy as np\n",
        "from PIL import Image, ImageFile, ImageOps\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "import math\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "Image.MAX_IMAGE_PIXELS = None\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "# from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import torch packages that help us define our network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms.transforms import Normalize\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "# Package that allows us to summarize our network\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# dir_path = os.path.dirname(os.path.realpath(__file__))\n",
        "# parent_dir_path = os.path.abspath(os.path.join(dir_path, os.pardir))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr7V86XKjK08",
        "outputId": "7e247df4-605a-45a9-8732-c46bc1bf727f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Hirise_Image_Dataset(Dataset):\n",
        "    \"\"\"Hirise Image dataset.\"\"\"\n",
        "    def __init__(self,\n",
        "                 path_to_images,\n",
        "                 transform=None):\n",
        "        # ------------------------------------------------------------------------------\n",
        "        # path_to_images: where you put the image dataset\n",
        "        # transform:  data transform\n",
        "        # img_size: resize all images to a standard size\n",
        "        # ------------------------------------------------------------------------------\n",
        "\n",
        "        # Load all the images and their labels\n",
        "        self.dataset = datasets.ImageFolder(path_to_images, transform=transform)\n",
        "        self.len = len(self.dataset.imgs)\n",
        "        self.path_to_images = path_to_images\n",
        "\n",
        "        # ------------------------------------------------------------------------------\n",
        "        # Split the data into train and test data 80 : 20\n",
        "        # ------------------------------------------------------------------------------\n",
        "        # Calculate the lengths of the vectors\n",
        "        lengths = [int(np.ceil(len(self.dataset)*0.8)), int(np.floor(len(self.dataset)*0.2))]\n",
        "\n",
        "\n",
        "        # Extract the images and labels   \n",
        "        self.train_dataset, self.test_dataset = random_split(self.dataset, lengths)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of samples\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample, target = self.data[idx], self.data[idx]\n",
        "        sample = sample.view(1, 256, 256).float()/255.\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "            target = self.transform(target)\n",
        "        return sample, target"
      ],
      "metadata": {
        "id": "I-1eEjXNlJUu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Data_Preparation:\n",
        "    \"\"\"Class that allows for data prepartion as part of the preprocessing of the hirise images. \"\"\"\n",
        "    def remove_background(self,file_name):\n",
        "        src = cv2.imread(file_name, 1)\n",
        "        tmp = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n",
        "        _,alpha = cv2.threshold(tmp,0,255,cv2.THRESH_BINARY)\n",
        "        b, g, r = cv2.split(src)\n",
        "        rgba = [b,g,r, alpha]\n",
        "        dst = cv2.merge(rgba,4)\n",
        "        if np.allclose(np.asarray(dst), 0):\n",
        "            os.remove(file_name)\n",
        "        else:\n",
        "            cv2.imwrite(file_name, dst)\n",
        "\n",
        "    def resize_image(self, folder_path, resized_images_folder_path, pixel_length_cm = 250):\n",
        "        reduce_factor = 25/pixel_length_cm\n",
        "        imgfiles = glob(f\"{folder_path}/*.IMG\")\n",
        "\n",
        "        # Convert to PIL Imgae\n",
        "        img_list = []\n",
        "        for img in tqdm(imgfiles):\n",
        "            img_list.append(Image.open(img))\n",
        "\n",
        "        if os.path.isdir(resized_images_folder_path):\n",
        "            os.chdir(resized_images_folder_path)\n",
        "        else:\n",
        "            os.makedirs(resized_images_folder_path)\n",
        "            os.chdir(resized_images_folder_path)\n",
        "\n",
        "        for im,name in tqdm(zip(img_list,imgfiles)):         \n",
        "            resized_im = im.resize((round(im.size[0]*reduce_factor), round(im.size[1]*reduce_factor)))\n",
        "            print(\"Hello\")\n",
        "            try:\n",
        "              print(name)\n",
        "              resized_im.save(name.split('/')[-1]+'_resizedimage.jpg')\n",
        "            except:\n",
        "              pass\n",
        "            \n",
        "    def tile_images(self, folder_path,image_directory, image_size_pixels, resized = True,remove_background = True):\n",
        "        if resized:\n",
        "            imgfiles = glob(f\"{folder_path}/*.jpg\")\n",
        "        else:\n",
        "            imgfiles = glob(f\"{folder_path}/*.IMG\")\n",
        "        # Convert to PIL Imgae\n",
        "        img_list = []\n",
        "        for img in imgfiles:\n",
        "            img_list.append(Image.open(img))\n",
        "\n",
        "        if os.path.isdir(image_directory):\n",
        "            os.chdir(image_directory)\n",
        "        else:\n",
        "            os.makedirs(image_directory)\n",
        "            os.chdir(image_directory)\n",
        "\n",
        "        for img,name in tqdm(zip(img_list,imgfiles)):\n",
        "            try:\n",
        "                im = np.asarray(img)\n",
        "                for r in range(0,math.ceil(im.shape[0]),image_size_pixels):\n",
        "                    for c in range(0,math.ceil(im.shape[1]),image_size_pixels):\n",
        "                            f_name = name.split('/')[-1].split('.')[0] + f\"_{r}_{c}.jpg\"\n",
        "                            cv2.imwrite(str(f_name), im[r:r+image_size_pixels, c:c+image_size_pixels,:] )\n",
        "                            if remove_background:\n",
        "                                Data_Preparation.remove_background(self,file_name =f_name)\n",
        "            except:\n",
        "                pass                \n",
        "\n",
        "        # sys.path.insert(0, parent_dir_path)\n",
        "\n",
        "    def convert_to_grayscale(self, folder_path,image_directory, remove_background = True):\n",
        "        imgfiles = glob(f\"{folder_path}/*.jpg\")\n",
        "\n",
        "        im_list = []\n",
        "        # Convert to PIL Imgae\n",
        "        for img in imgfiles:\n",
        "            im_list.append(cv2.imread(img, 1))\n",
        "        \n",
        "        if os.path.isdir(image_directory):\n",
        "            os.chdir(image_directory)\n",
        "        else:\n",
        "            os.makedirs(image_directory)\n",
        "            os.chdir(image_directory)\n",
        "\n",
        "        for img,name in zip(im_list,imgfiles):\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            f_name = \"gray_\" + name.split('\\\\')[1]\n",
        "            try:\n",
        "                cv2.imwrite(f_name, gray)\n",
        "            except:\n",
        "                pass\n",
        "            if remove_background:\n",
        "                Data_Preparation.remove_background(self,file_name =f_name)\n",
        "        # sys.path.insert(0, parent_dir_path)\n",
        "\n",
        "    def remove_image_with_empty_pixels(self, folder_path, max_percentage_empty_space = 20):\n",
        "        imgfiles = glob(f\"{folder_path}/*.jpg\")\n",
        "\n",
        "        if os.path.isdir(folder_path):\n",
        "            os.chdir(folder_path)\n",
        "        else:\n",
        "            print(\"ERROR!!\")\n",
        "\n",
        "        for f_name in tqdm(imgfiles):\n",
        "            empty = 0\n",
        "            img = Image.open(f_name.split('\\\\')[1])\n",
        "            width, height = img.width, img.height\n",
        "            total = width * height\n",
        "            for pixel in img.getdata():\n",
        "                if pixel == (0,0,0,0) or pixel == (0,0,0):            \n",
        "                    empty += 1\n",
        "            percent = round((empty * 100.0/total),1)\n",
        "            if(percent >= max_percentage_empty_space):            \n",
        "                os.remove(f_name.split('\\\\')[1])\n",
        "        # sys.path.insert(0, parent_dir_path)\n",
        "\n",
        "    def get_image_dataset(self,f_path,  transform_data =  None ):\n",
        "        if not transform_data:\n",
        "            transform_data = transforms.Compose([transforms.ToTensor()])\n",
        "        # transform_data= transforms.Compose([transforms.ToTensor(), transforms.Grayscale(num_output_channels=1)])\n",
        "        dataset = Hirise_Image_Dataset(path_to_images = f_path, transform = transform_data)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def get_train_test_val_tensors(self, dataset):\n",
        "            m=len(dataset.train_dataset)\n",
        "\n",
        "            train_ds, val_ds = random_split(dataset.train_dataset, [math.floor(m-m*0.2), math.ceil(m*0.2)])\n",
        "            # ------------------Training Data ----------------------------------------------\n",
        "            # Empty lists to store the training data\n",
        "            train_list = []\n",
        "            # Append from the MedicalMNIST Object the training target and labels\n",
        "            for data in train_ds:\n",
        "                train_list.append(data[0])\n",
        "\n",
        "            train_tensor = torch.Tensor(len(train_list))\n",
        "            try :\n",
        "                torch.cat(train_list,out = train_tensor)\n",
        "            except :\n",
        "                pass\n",
        "            # ------------------- --- Test Data ---------------------------------------------\n",
        "            # Empty lists to store the test data\n",
        "            test_list = []\n",
        "            for data in dataset.test_dataset:\n",
        "                test_list.append(data[0])\n",
        "\n",
        "            test_tensor = torch.Tensor(len(test_list))\n",
        "            try:\n",
        "                torch.cat(test_list,out = test_tensor)\n",
        "            except :\n",
        "                pass\n",
        "            # ------------------- --- Val Data ---------------------------------------------\n",
        "            # Empty lists to store the test data\n",
        "            val_list = []\n",
        "            for data in val_ds:\n",
        "                val_list.append(data[0])\n",
        "\n",
        "            val_tensor = torch.Tensor(len(val_list))\n",
        "\n",
        "            try:\n",
        "                torch.cat(val_list,out = val_tensor)\n",
        "            except :\n",
        "                pass\n",
        "            return  train_tensor, test_tensor, val_tensor\n",
        "\n",
        "    def get_train_test_val_dataloader(self, train_data, test_data, val_data,  b_size = 128):\n",
        "        # Create TorchTensor Datasets containing training_data, testing_data, validation_data\n",
        "        training_data = TensorDataset(train_data,train_data.long() )\n",
        "        validation_data = TensorDataset(val_data,val_data.long() )\n",
        "        testing_data = TensorDataset(test_data, test_data.long())\n",
        "        train_loader = DataLoader(dataset = training_data, batch_size=b_size)\n",
        "        valid_loader = DataLoader(dataset = validation_data, batch_size=b_size)\n",
        "        test_loader = DataLoader(dataset = testing_data, batch_size=b_size,shuffle=True)\n",
        "        return train_loader,  test_loader, valid_loader\n",
        "\n",
        "    def show_training_data(self, dataset, grid_rows=5, grid_columns=5):\n",
        "        \"\"\" Prints the traning data in a grid\"\"\"\n",
        "        # Set up axes and subplots\n",
        "        fig, axarr = plt.subplots(grid_rows, grid_columns, figsize=(10, 10))\n",
        "\n",
        "        # Loops to run over the grid\n",
        "        for i in range(grid_rows):\n",
        "            for j in range(grid_columns):\n",
        "\n",
        "                # Generate a random index in the training dataset\n",
        "                idx = random.randint(0, len(dataset.train_dataset))\n",
        "\n",
        "                # Get the sample and target fromthe traiig dataset\n",
        "                sample, _ = dataset.train_dataset[idx]\n",
        "\n",
        "                try:\n",
        "                    # Exception handling - if it is PIL\n",
        "                    axarr[i][j].imshow(sample, cmap = \"gray\")\n",
        "                except:\n",
        "                    # If tensor of shape CHW\n",
        "                    axarr[i][j].imshow(sample.permute(1,2,0), cmap = \"gray\") \n",
        "                # # Get the classes of the target data\n",
        "                # target_name = dataset.dataset.targets[target]\n",
        "                # # Label each image eith the target name and the class it belongs to\n",
        "                # axarr[i][j].set_title(\"%s (%i)\"%(target_name, target))\n",
        "        # Deine the grid layout and padding\n",
        "        \n",
        "        fig.tight_layout(pad=1)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "7nrbn76mihjU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class CAE_Encoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, encoded_space_dim,fc2_input_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        ### Convolutional section\n",
        "        self.encoder_cnn = nn.Sequential(\n",
        "            # First convolutional layer\n",
        "            nn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
        "            #nn.BatchNorm2d(8),\n",
        "            nn.ReLU(True),\n",
        "            # Second convolutional layer\n",
        "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(True),\n",
        "            # Third convolutional layer\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=0),\n",
        "            #nn.BatchNorm2d(32),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        \n",
        "        ### Flatten layer\n",
        "        self.flatten = nn.Flatten(start_dim=1)\n",
        "\n",
        "        ### Linear section\n",
        "        self.encoder_lin = nn.Sequential(\n",
        "            # First linear layer\n",
        "            nn.Linear(3 * 3 * 32, 128),\n",
        "            nn.ReLU(True),\n",
        "            # Second linear layer\n",
        "            nn.Linear(128, encoded_space_dim)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Apply convolutions\n",
        "        x = self.encoder_cnn(x)\n",
        "        # Flatten\n",
        "        x = self.flatten(x)\n",
        "        # # Apply linear layers\n",
        "        x = self.encoder_lin(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ue3tZYcYe1pN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CAE_Decoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, encoded_space_dim,fc2_input_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        ### Linear section\n",
        "        self.decoder_lin = nn.Sequential(\n",
        "            # First linear layer\n",
        "            nn.Linear(encoded_space_dim, 128),\n",
        "            nn.ReLU(True),\n",
        "            # Second linear layer\n",
        "            nn.Linear(128, 3 * 3 * 32),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        ### Unflatten\n",
        "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(32, 3, 3))\n",
        "\n",
        "        ### Convolutional section\n",
        "        self.decoder_conv = nn.Sequential(\n",
        "            # First transposed convolution\n",
        "            nn.ConvTranspose2d(32, 16, 3, stride=2, output_padding=0),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(True),\n",
        "            # Second transposed convolution\n",
        "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(True),\n",
        "            # Third transposed convolution\n",
        "            nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Apply linear layers\n",
        "        x = self.decoder_lin(x)\n",
        "        # Unflatten\n",
        "        x = self.unflatten(x)\n",
        "        # Apply transposed convolutions\n",
        "        x = self.decoder_conv(x)\n",
        "        # Apply a sigmoid to force the output to be between 0 and 1 (valid pixel values)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Ia81GBElcL8H"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "latent_dims = 10\n",
        "num_epochs = 50\n",
        "batch_size = 128\n",
        "capacity = 64\n",
        "learning_rate = 1e-3\n",
        "use_gpu = True\n",
        "\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "### Define an optimizer (both for the encoder and the decoder!)\n",
        "lr= 0.001\n",
        "\n",
        "### Set the random seed for reproducible results\n",
        "torch.manual_seed(0)\n",
        "\n",
        "### Initialize the two networks\n",
        "d = 10\n",
        "\n",
        "#model = Autoencoder(encoded_space_dim=encoded_space_dim)\n",
        "encoder = CAE_Encoder(encoded_space_dim=d,fc2_input_dim=256)\n",
        "decoder = CAE_Decoder(encoded_space_dim=d,fc2_input_dim=256)\n",
        "params_to_optimize = [\n",
        "    {'params': encoder.parameters()},\n",
        "    {'params': decoder.parameters()}\n",
        "]\n",
        "\n",
        "optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n",
        "\n",
        "# Check if the GPU is available\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f'Selected device: {device}')\n",
        "\n",
        "# Move both the encoder and the decoder to the selected device\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFzS_CQ9cTDt",
        "outputId": "ce094564-4bc3-4b48-f922-2d86c7a95264"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CAE_Decoder(\n",
              "  (decoder_lin): Sequential(\n",
              "    (0): Linear(in_features=10, out_features=128, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Linear(in_features=128, out_features=288, bias=True)\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (unflatten): Unflatten(dim=1, unflattened_size=(32, 3, 3))\n",
              "  (decoder_conv): Sequential(\n",
              "    (0): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2))\n",
              "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): ConvTranspose2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "    (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): ConvTranspose2d(8, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/MyDrive/Images/img/'\n",
        "num_epochs = 30\n",
        "diz_loss = {'train_loss':[],'val_loss':[]}\n",
        "transform1= transforms.Compose([transforms.ToTensor(), transforms.Resize((28, 28)), transforms.Grayscale(num_output_channels=1)])\n",
        "\n",
        "dp = Data_Preparation()\n",
        "\n",
        "dataset1 = dp.get_image_dataset(f_path = folder_path, transform_data = transform1)\n",
        "tr,tst,val = dp.get_train_test_val_tensors(dataset = dataset1)\n",
        "train_loader,test_loader, val_l = dp.get_train_test_val_dataloader(tr,tst,val)"
      ],
      "metadata": {
        "id": "U_pvutVecpW4"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, _ = dataset1.train_dataset[0]\n",
        "img = img.unsqueeze(0).to(device) # Add the batch dimension in the first axis\n",
        "print('Original image shape:', img.shape)\n",
        "\n",
        "\n",
        "img_enc = encoder(img)\n",
        "print('Encoded image shape:', img_enc.shape)\n",
        "\n",
        "# Decode the image\n",
        "dec_img = decoder(img_enc)\n",
        "#dec_img = model(img)\n",
        "print('Decoded image shape:', dec_img.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4ATgexqdXNB",
        "outputId": "0f344619-ef59-465a-c5e1-7f3dc3f61706"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original image shape: torch.Size([1, 1, 28, 28])\n",
            "Encoded image shape: torch.Size([1, 10])\n",
            "Decoded image shape: torch.Size([1, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Training function\n",
        "def train_CAE(encoder, decoder, device, dataloader, loss_fn, optimizer):\n",
        "    # Set train mode for both the encoder and the decoder\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    train_loss = []\n",
        "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
        "    for image_batch, _ in dataloader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
        "        # Move tensor to the proper device\n",
        "        image_batch = image_batch.to(device)\n",
        "        # Encode data\n",
        "        encoded_data = encoder(image_batch)\n",
        "        # Decode data\n",
        "        decoded_data = decoder(encoded_data)\n",
        "        # Evaluate loss\n",
        "        loss = loss_fn(decoded_data, image_batch)\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Print batch loss\n",
        "        print('\\t partial train loss (single batch): %f' % (loss.data))\n",
        "        train_loss.append(loss.detach().cpu().numpy())\n",
        "\n",
        "    return np.mean(train_loss)\n",
        "      ### Training function\n",
        "### Training function\n",
        "def train_epoch(encoder, decoder, device, dataloader, loss_fn, optimizer):\n",
        "    # Set train mode for both the encoder and the decoder\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    train_loss = []\n",
        "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
        "    for image_batch, _ in dataloader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
        "        image_batch = image_batch.unsqueeze(0).permute(1,0,2,3)\n",
        "        # Move tensor to the proper device\n",
        "        image_batch = image_batch.to(device)\n",
        "        # Encode data\n",
        "        encoded_data = encoder(image_batch)\n",
        "        # Decode data\n",
        "        decoded_data = decoder(encoded_data)\n",
        "        # Evaluate loss\n",
        "        loss = loss_fn(decoded_data, image_batch)\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Print batch loss\n",
        "        print('\\t partial train loss (single batch): %f' % (loss.data))\n",
        "        train_loss.append(loss.detach().cpu().numpy())\n",
        "\n",
        "    return np.mean(train_loss)\n",
        "\n",
        "\n",
        "def test_epoch(encoder, decoder, device, dataloader, loss_fn):\n",
        "    # Set evaluation mode for encoder and decoder\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    with torch.no_grad(): # No need to track the gradients\n",
        "        # Define the lists to store the outputs for each batch\n",
        "        conc_out = []\n",
        "        conc_label = []\n",
        "        for image_batch, _ in dataloader:\n",
        "            # Move tensor to the proper device\n",
        "            image_batch = image_batch.unsqueeze(0).permute(1,0,2,3)\n",
        "            image_batch = image_batch.to(device)\n",
        "            # Encode data\n",
        "            encoded_data = encoder(image_batch)\n",
        "            # Decode data\n",
        "            decoded_data = decoder(encoded_data)\n",
        "            # Append the network output and the original image to the lists\n",
        "            conc_out.append(decoded_data.cpu())\n",
        "            conc_label.append(image_batch.cpu())\n",
        "        # Create a single tensor with all the values in the lists\n",
        "        conc_out = torch.cat(conc_out)\n",
        "        conc_label = torch.cat(conc_label) \n",
        "        # Evaluate global loss\n",
        "        val_loss = loss_fn(conc_out, conc_label)\n",
        "    return val_loss.data\n",
        "  \n",
        "\n",
        "\n",
        "def plot_ae_outputs(encoder,decoder,n=5):\n",
        "    plt.figure(figsize=(10,4.5))\n",
        "    for i in range(n):\n",
        "      ax = plt.subplot(2,n,i+1)\n",
        "      img = dataset1.train_dataset[i][0].unsqueeze(0).to(device)\n",
        "      encoder.eval()\n",
        "      decoder.eval()\n",
        "      with torch.no_grad():\n",
        "         rec_img  = decoder(encoder(img))\n",
        "      plt.imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
        "      ax.get_xaxis().set_visible(False)\n",
        "      ax.get_yaxis().set_visible(False)  \n",
        "      if i == n//2:\n",
        "        ax.set_title('Original images')\n",
        "      ax = plt.subplot(2, n, i + 1 + n)\n",
        "      plt.imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')  \n",
        "      ax.get_xaxis().set_visible(False)\n",
        "      ax.get_yaxis().set_visible(False)  \n",
        "      if i == n//2:\n",
        "         ax.set_title('Reconstructed images')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "wb6fq1hOcY1u"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 300\n",
        "diz_loss = {'train_loss':[],'val_loss':[]}\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss =train_epoch(encoder,decoder,device,train_loader,loss_fn,optim)\n",
        "    val_loss = test_epoch(encoder,decoder,device,test_loader,loss_fn)\n",
        "    print('\\n EPOCH {}/{} \\t train loss {} \\t val loss {}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
        "    diz_loss['train_loss'].append(train_loss)\n",
        "    diz_loss['val_loss'].append(val_loss)\n",
        "\n",
        "\n",
        "\n",
        "plot_ae_outputs(encoder,decoder,n=10)\n"
      ],
      "metadata": {
        "id": "7ThfWASNiQm-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "408cf77d-83b8-4998-da8e-ac9e657a3e2b"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t partial train loss (single batch): 0.054633\n",
            "\t partial train loss (single batch): 0.039012\n",
            "\n",
            " EPOCH 1/300 \t train loss 0.04682251811027527 \t val loss 0.0466066412627697\n",
            "\t partial train loss (single batch): 0.052526\n",
            "\t partial train loss (single batch): 0.037427\n",
            "\n",
            " EPOCH 2/300 \t train loss 0.04497649520635605 \t val loss 0.0443735234439373\n",
            "\t partial train loss (single batch): 0.050528\n",
            "\t partial train loss (single batch): 0.035935\n",
            "\n",
            " EPOCH 3/300 \t train loss 0.04323141276836395 \t val loss 0.04223394766449928\n",
            "\t partial train loss (single batch): 0.048614\n",
            "\t partial train loss (single batch): 0.034520\n",
            "\n",
            " EPOCH 4/300 \t train loss 0.04156713932752609 \t val loss 0.0403413325548172\n",
            "\t partial train loss (single batch): 0.046813\n",
            "\t partial train loss (single batch): 0.033177\n",
            "\n",
            " EPOCH 5/300 \t train loss 0.03999496251344681 \t val loss 0.03842907398939133\n",
            "\t partial train loss (single batch): 0.045076\n",
            "\t partial train loss (single batch): 0.031920\n",
            "\n",
            " EPOCH 6/300 \t train loss 0.03849766403436661 \t val loss 0.036674294620752335\n",
            "\t partial train loss (single batch): 0.043421\n",
            "\t partial train loss (single batch): 0.030714\n",
            "\n",
            " EPOCH 7/300 \t train loss 0.037067048251628876 \t val loss 0.03503888100385666\n",
            "\t partial train loss (single batch): 0.041852\n",
            "\t partial train loss (single batch): 0.029594\n",
            "\n",
            " EPOCH 8/300 \t train loss 0.035723187029361725 \t val loss 0.03367312625050545\n",
            "\t partial train loss (single batch): 0.040397\n",
            "\t partial train loss (single batch): 0.028545\n",
            "\n",
            " EPOCH 9/300 \t train loss 0.03447078540921211 \t val loss 0.03245134651660919\n",
            "\t partial train loss (single batch): 0.039054\n",
            "\t partial train loss (single batch): 0.027590\n",
            "\n",
            " EPOCH 10/300 \t train loss 0.03332187235355377 \t val loss 0.03135380521416664\n",
            "\t partial train loss (single batch): 0.037773\n",
            "\t partial train loss (single batch): 0.026689\n",
            "\n",
            " EPOCH 11/300 \t train loss 0.032231077551841736 \t val loss 0.030472997575998306\n",
            "\t partial train loss (single batch): 0.036556\n",
            "\t partial train loss (single batch): 0.025814\n",
            "\n",
            " EPOCH 12/300 \t train loss 0.03118547424674034 \t val loss 0.029661109670996666\n",
            "\t partial train loss (single batch): 0.035425\n",
            "\t partial train loss (single batch): 0.024985\n",
            "\n",
            " EPOCH 13/300 \t train loss 0.030205290764570236 \t val loss 0.02871512435376644\n",
            "\t partial train loss (single batch): 0.034341\n",
            "\t partial train loss (single batch): 0.024212\n",
            "\n",
            " EPOCH 14/300 \t train loss 0.0292764063924551 \t val loss 0.02762443758547306\n",
            "\t partial train loss (single batch): 0.033304\n",
            "\t partial train loss (single batch): 0.023497\n",
            "\n",
            " EPOCH 15/300 \t train loss 0.028400365263223648 \t val loss 0.026646168902516365\n",
            "\t partial train loss (single batch): 0.032325\n",
            "\t partial train loss (single batch): 0.022810\n",
            "\n",
            " EPOCH 16/300 \t train loss 0.02756744995713234 \t val loss 0.025759343057870865\n",
            "\t partial train loss (single batch): 0.031400\n",
            "\t partial train loss (single batch): 0.022159\n",
            "\n",
            " EPOCH 17/300 \t train loss 0.026779575273394585 \t val loss 0.0251218993216753\n",
            "\t partial train loss (single batch): 0.030531\n",
            "\t partial train loss (single batch): 0.021523\n",
            "\n",
            " EPOCH 18/300 \t train loss 0.026027176529169083 \t val loss 0.024480320513248444\n",
            "\t partial train loss (single batch): 0.029698\n",
            "\t partial train loss (single batch): 0.020929\n",
            "\n",
            " EPOCH 19/300 \t train loss 0.02531353197991848 \t val loss 0.023776860907673836\n",
            "\t partial train loss (single batch): 0.028890\n",
            "\t partial train loss (single batch): 0.020373\n",
            "\n",
            " EPOCH 20/300 \t train loss 0.024631887674331665 \t val loss 0.023133734241127968\n",
            "\t partial train loss (single batch): 0.028139\n",
            "\t partial train loss (single batch): 0.019825\n",
            "\n",
            " EPOCH 21/300 \t train loss 0.023982156068086624 \t val loss 0.022468147799372673\n",
            "\t partial train loss (single batch): 0.027419\n",
            "\t partial train loss (single batch): 0.019324\n",
            "\n",
            " EPOCH 22/300 \t train loss 0.023371174931526184 \t val loss 0.021820835769176483\n",
            "\t partial train loss (single batch): 0.026730\n",
            "\t partial train loss (single batch): 0.018845\n",
            "\n",
            " EPOCH 23/300 \t train loss 0.02278764359652996 \t val loss 0.021350059658288956\n",
            "\t partial train loss (single batch): 0.026094\n",
            "\t partial train loss (single batch): 0.018378\n",
            "\n",
            " EPOCH 24/300 \t train loss 0.022236116230487823 \t val loss 0.020711630582809448\n",
            "\t partial train loss (single batch): 0.025461\n",
            "\t partial train loss (single batch): 0.017961\n",
            "\n",
            " EPOCH 25/300 \t train loss 0.02171109989285469 \t val loss 0.020277369767427444\n",
            "\t partial train loss (single batch): 0.024891\n",
            "\t partial train loss (single batch): 0.017529\n",
            "\n",
            " EPOCH 26/300 \t train loss 0.02120974287390709 \t val loss 0.019901655614376068\n",
            "\t partial train loss (single batch): 0.024335\n",
            "\t partial train loss (single batch): 0.017163\n",
            "\n",
            " EPOCH 27/300 \t train loss 0.020748700946569443 \t val loss 0.01928483322262764\n",
            "\t partial train loss (single batch): 0.023771\n",
            "\t partial train loss (single batch): 0.016777\n",
            "\n",
            " EPOCH 28/300 \t train loss 0.020273640751838684 \t val loss 0.019163278862833977\n",
            "\t partial train loss (single batch): 0.023304\n",
            "\t partial train loss (single batch): 0.016407\n",
            "\n",
            " EPOCH 29/300 \t train loss 0.01985577866435051 \t val loss 0.01867823116481304\n",
            "\t partial train loss (single batch): 0.022790\n",
            "\t partial train loss (single batch): 0.016115\n",
            "\n",
            " EPOCH 30/300 \t train loss 0.01945246383547783 \t val loss 0.018328940495848656\n",
            "\t partial train loss (single batch): 0.022319\n",
            "\t partial train loss (single batch): 0.015765\n",
            "\n",
            " EPOCH 31/300 \t train loss 0.019042205065488815 \t val loss 0.018043171614408493\n",
            "\t partial train loss (single batch): 0.021892\n",
            "\t partial train loss (single batch): 0.015474\n",
            "\n",
            " EPOCH 32/300 \t train loss 0.01868297904729843 \t val loss 0.017584139481186867\n",
            "\t partial train loss (single batch): 0.021407\n",
            "\t partial train loss (single batch): 0.015199\n",
            "\n",
            " EPOCH 33/300 \t train loss 0.018302863463759422 \t val loss 0.017338814213871956\n",
            "\t partial train loss (single batch): 0.021026\n",
            "\t partial train loss (single batch): 0.014865\n",
            "\n",
            " EPOCH 34/300 \t train loss 0.017945069819688797 \t val loss 0.017094921320676804\n",
            "\t partial train loss (single batch): 0.020668\n",
            "\t partial train loss (single batch): 0.014620\n",
            "\n",
            " EPOCH 35/300 \t train loss 0.017643969506025314 \t val loss 0.01659814640879631\n",
            "\t partial train loss (single batch): 0.020204\n",
            "\t partial train loss (single batch): 0.014401\n",
            "\n",
            " EPOCH 36/300 \t train loss 0.017302386462688446 \t val loss 0.016372526064515114\n",
            "\t partial train loss (single batch): 0.019846\n",
            "\t partial train loss (single batch): 0.014093\n",
            "\n",
            " EPOCH 37/300 \t train loss 0.016969747841358185 \t val loss 0.01613732986152172\n",
            "\t partial train loss (single batch): 0.019523\n",
            "\t partial train loss (single batch): 0.013806\n",
            "\n",
            " EPOCH 38/300 \t train loss 0.016664568334817886 \t val loss 0.015783345326781273\n",
            "\t partial train loss (single batch): 0.019207\n",
            "\t partial train loss (single batch): 0.013510\n",
            "\n",
            " EPOCH 39/300 \t train loss 0.016358736902475357 \t val loss 0.015632325783371925\n",
            "\t partial train loss (single batch): 0.018905\n",
            "\t partial train loss (single batch): 0.013303\n",
            "\n",
            " EPOCH 40/300 \t train loss 0.01610400900244713 \t val loss 0.015096831135451794\n",
            "\t partial train loss (single batch): 0.018484\n",
            "\t partial train loss (single batch): 0.013108\n",
            "\n",
            " EPOCH 41/300 \t train loss 0.015796441584825516 \t val loss 0.014934796839952469\n",
            "\t partial train loss (single batch): 0.018151\n",
            "\t partial train loss (single batch): 0.012841\n",
            "\n",
            " EPOCH 42/300 \t train loss 0.015496280044317245 \t val loss 0.014706847257912159\n",
            "\t partial train loss (single batch): 0.017860\n",
            "\t partial train loss (single batch): 0.012626\n",
            "\n",
            " EPOCH 43/300 \t train loss 0.015243308618664742 \t val loss 0.014341249130666256\n",
            "\t partial train loss (single batch): 0.017532\n",
            "\t partial train loss (single batch): 0.012417\n",
            "\n",
            " EPOCH 44/300 \t train loss 0.014974607154726982 \t val loss 0.01418802049010992\n",
            "\t partial train loss (single batch): 0.017268\n",
            "\t partial train loss (single batch): 0.012193\n",
            "\n",
            " EPOCH 45/300 \t train loss 0.01473055686801672 \t val loss 0.013894131407141685\n",
            "\t partial train loss (single batch): 0.016971\n",
            "\t partial train loss (single batch): 0.012033\n",
            "\n",
            " EPOCH 46/300 \t train loss 0.014501924626529217 \t val loss 0.013669680804014206\n",
            "\t partial train loss (single batch): 0.016709\n",
            "\t partial train loss (single batch): 0.011786\n",
            "\n",
            " EPOCH 47/300 \t train loss 0.014247508719563484 \t val loss 0.013501990586519241\n",
            "\t partial train loss (single batch): 0.016434\n",
            "\t partial train loss (single batch): 0.011632\n",
            "\n",
            " EPOCH 48/300 \t train loss 0.01403273269534111 \t val loss 0.013234364800155163\n",
            "\t partial train loss (single batch): 0.016159\n",
            "\t partial train loss (single batch): 0.011429\n",
            "\n",
            " EPOCH 49/300 \t train loss 0.013793935999274254 \t val loss 0.013054808601737022\n",
            "\t partial train loss (single batch): 0.015895\n",
            "\t partial train loss (single batch): 0.011261\n",
            "\n",
            " EPOCH 50/300 \t train loss 0.013578212819993496 \t val loss 0.012905290350317955\n",
            "\t partial train loss (single batch): 0.015641\n",
            "\t partial train loss (single batch): 0.011032\n",
            "\n",
            " EPOCH 51/300 \t train loss 0.013336433097720146 \t val loss 0.012752117589116096\n",
            "\t partial train loss (single batch): 0.015434\n",
            "\t partial train loss (single batch): 0.010854\n",
            "\n",
            " EPOCH 52/300 \t train loss 0.013143706135451794 \t val loss 0.012546861544251442\n",
            "\t partial train loss (single batch): 0.015183\n",
            "\t partial train loss (single batch): 0.010638\n",
            "\n",
            " EPOCH 53/300 \t train loss 0.012910640798509121 \t val loss 0.01249059196561575\n",
            "\t partial train loss (single batch): 0.014999\n",
            "\t partial train loss (single batch): 0.010503\n",
            "\n",
            " EPOCH 54/300 \t train loss 0.012750968337059021 \t val loss 0.012185896746814251\n",
            "\t partial train loss (single batch): 0.014728\n",
            "\t partial train loss (single batch): 0.010330\n",
            "\n",
            " EPOCH 55/300 \t train loss 0.01252889633178711 \t val loss 0.012209602631628513\n",
            "\t partial train loss (single batch): 0.014555\n",
            "\t partial train loss (single batch): 0.010213\n",
            "\n",
            " EPOCH 56/300 \t train loss 0.012384378351271152 \t val loss 0.011838930658996105\n",
            "\t partial train loss (single batch): 0.014270\n",
            "\t partial train loss (single batch): 0.010042\n",
            "\n",
            " EPOCH 57/300 \t train loss 0.012155888602137566 \t val loss 0.011872963048517704\n",
            "\t partial train loss (single batch): 0.014130\n",
            "\t partial train loss (single batch): 0.009867\n",
            "\n",
            " EPOCH 58/300 \t train loss 0.011998729780316353 \t val loss 0.011546410620212555\n",
            "\t partial train loss (single batch): 0.013877\n",
            "\t partial train loss (single batch): 0.009725\n",
            "\n",
            " EPOCH 59/300 \t train loss 0.011801362037658691 \t val loss 0.011441065929830074\n",
            "\t partial train loss (single batch): 0.013683\n",
            "\t partial train loss (single batch): 0.009577\n",
            "\n",
            " EPOCH 60/300 \t train loss 0.011630346067249775 \t val loss 0.011206632480025291\n",
            "\t partial train loss (single batch): 0.013424\n",
            "\t partial train loss (single batch): 0.009513\n",
            "\n",
            " EPOCH 61/300 \t train loss 0.011468527838587761 \t val loss 0.010986614041030407\n",
            "\t partial train loss (single batch): 0.013232\n",
            "\t partial train loss (single batch): 0.009366\n",
            "\n",
            " EPOCH 62/300 \t train loss 0.011298805475234985 \t val loss 0.010976343415677547\n",
            "\t partial train loss (single batch): 0.013063\n",
            "\t partial train loss (single batch): 0.009271\n",
            "\n",
            " EPOCH 63/300 \t train loss 0.011167052201926708 \t val loss 0.010678380727767944\n",
            "\t partial train loss (single batch): 0.012901\n",
            "\t partial train loss (single batch): 0.009112\n",
            "\n",
            " EPOCH 64/300 \t train loss 0.011006658896803856 \t val loss 0.01085341814905405\n",
            "\t partial train loss (single batch): 0.012795\n",
            "\t partial train loss (single batch): 0.008954\n",
            "\n",
            " EPOCH 65/300 \t train loss 0.01087440736591816 \t val loss 0.010470056906342506\n",
            "\t partial train loss (single batch): 0.012629\n",
            "\t partial train loss (single batch): 0.008788\n",
            "\n",
            " EPOCH 66/300 \t train loss 0.010708389803767204 \t val loss 0.010655215941369534\n",
            "\t partial train loss (single batch): 0.012480\n",
            "\t partial train loss (single batch): 0.008648\n",
            "\n",
            " EPOCH 67/300 \t train loss 0.010563993826508522 \t val loss 0.010226928628981113\n",
            "\t partial train loss (single batch): 0.012249\n",
            "\t partial train loss (single batch): 0.008572\n",
            "\n",
            " EPOCH 68/300 \t train loss 0.010410482063889503 \t val loss 0.010273582302033901\n",
            "\t partial train loss (single batch): 0.012043\n",
            "\t partial train loss (single batch): 0.008475\n",
            "\n",
            " EPOCH 69/300 \t train loss 0.010259099304676056 \t val loss 0.009973004460334778\n",
            "\t partial train loss (single batch): 0.011828\n",
            "\t partial train loss (single batch): 0.008378\n",
            "\n",
            " EPOCH 70/300 \t train loss 0.010103074833750725 \t val loss 0.009955720975995064\n",
            "\t partial train loss (single batch): 0.011681\n",
            "\t partial train loss (single batch): 0.008250\n",
            "\n",
            " EPOCH 71/300 \t train loss 0.009965664707124233 \t val loss 0.009810810908675194\n",
            "\t partial train loss (single batch): 0.011534\n",
            "\t partial train loss (single batch): 0.008181\n",
            "\n",
            " EPOCH 72/300 \t train loss 0.009857546538114548 \t val loss 0.009753975085914135\n",
            "\t partial train loss (single batch): 0.011412\n",
            "\t partial train loss (single batch): 0.008099\n",
            "\n",
            " EPOCH 73/300 \t train loss 0.00975579209625721 \t val loss 0.009552769362926483\n",
            "\t partial train loss (single batch): 0.011247\n",
            "\t partial train loss (single batch): 0.008096\n",
            "\n",
            " EPOCH 74/300 \t train loss 0.00967152789235115 \t val loss 0.009600427933037281\n",
            "\t partial train loss (single batch): 0.011147\n",
            "\t partial train loss (single batch): 0.008007\n",
            "\n",
            " EPOCH 75/300 \t train loss 0.009576743468642235 \t val loss 0.00938380416482687\n",
            "\t partial train loss (single batch): 0.011005\n",
            "\t partial train loss (single batch): 0.007976\n",
            "\n",
            " EPOCH 76/300 \t train loss 0.009490545839071274 \t val loss 0.009446068666875362\n",
            "\t partial train loss (single batch): 0.010887\n",
            "\t partial train loss (single batch): 0.007858\n",
            "\n",
            " EPOCH 77/300 \t train loss 0.009372534230351448 \t val loss 0.009233290329575539\n",
            "\t partial train loss (single batch): 0.010746\n",
            "\t partial train loss (single batch): 0.007796\n",
            "\n",
            " EPOCH 78/300 \t train loss 0.009270749986171722 \t val loss 0.009186862967908382\n",
            "\t partial train loss (single batch): 0.010568\n",
            "\t partial train loss (single batch): 0.007677\n",
            "\n",
            " EPOCH 79/300 \t train loss 0.009122440591454506 \t val loss 0.009047230705618858\n",
            "\t partial train loss (single batch): 0.010433\n",
            "\t partial train loss (single batch): 0.007544\n",
            "\n",
            " EPOCH 80/300 \t train loss 0.008988739922642708 \t val loss 0.008945053443312645\n",
            "\t partial train loss (single batch): 0.010338\n",
            "\t partial train loss (single batch): 0.007390\n",
            "\n",
            " EPOCH 81/300 \t train loss 0.00886411964893341 \t val loss 0.008920825086534023\n",
            "\t partial train loss (single batch): 0.010217\n",
            "\t partial train loss (single batch): 0.007280\n",
            "\n",
            " EPOCH 82/300 \t train loss 0.008748762309551239 \t val loss 0.008793325163424015\n",
            "\t partial train loss (single batch): 0.010113\n",
            "\t partial train loss (single batch): 0.007171\n",
            "\n",
            " EPOCH 83/300 \t train loss 0.00864163227379322 \t val loss 0.008735225535929203\n",
            "\t partial train loss (single batch): 0.009977\n",
            "\t partial train loss (single batch): 0.007116\n",
            "\n",
            " EPOCH 84/300 \t train loss 0.008546638302505016 \t val loss 0.008567096665501595\n",
            "\t partial train loss (single batch): 0.009830\n",
            "\t partial train loss (single batch): 0.007042\n",
            "\n",
            " EPOCH 85/300 \t train loss 0.008435827679932117 \t val loss 0.008544119074940681\n",
            "\t partial train loss (single batch): 0.009739\n",
            "\t partial train loss (single batch): 0.006970\n",
            "\n",
            " EPOCH 86/300 \t train loss 0.008354649879038334 \t val loss 0.008371932432055473\n",
            "\t partial train loss (single batch): 0.009588\n",
            "\t partial train loss (single batch): 0.006934\n",
            "\n",
            " EPOCH 87/300 \t train loss 0.008261214010417461 \t val loss 0.008336885832250118\n",
            "\t partial train loss (single batch): 0.009497\n",
            "\t partial train loss (single batch): 0.006850\n",
            "\n",
            " EPOCH 88/300 \t train loss 0.008173555135726929 \t val loss 0.008224749006330967\n",
            "\t partial train loss (single batch): 0.009364\n",
            "\t partial train loss (single batch): 0.006807\n",
            "\n",
            " EPOCH 89/300 \t train loss 0.008085157722234726 \t val loss 0.008150662295520306\n",
            "\t partial train loss (single batch): 0.009268\n",
            "\t partial train loss (single batch): 0.006688\n",
            "\n",
            " EPOCH 90/300 \t train loss 0.007977768778800964 \t val loss 0.008071050979197025\n",
            "\t partial train loss (single batch): 0.009173\n",
            "\t partial train loss (single batch): 0.006629\n",
            "\n",
            " EPOCH 91/300 \t train loss 0.007900912314653397 \t val loss 0.007994464598596096\n",
            "\t partial train loss (single batch): 0.009065\n",
            "\t partial train loss (single batch): 0.006537\n",
            "\n",
            " EPOCH 92/300 \t train loss 0.007800695486366749 \t val loss 0.0079292431473732\n",
            "\t partial train loss (single batch): 0.008980\n",
            "\t partial train loss (single batch): 0.006473\n",
            "\n",
            " EPOCH 93/300 \t train loss 0.007726144045591354 \t val loss 0.007856926880776882\n",
            "\t partial train loss (single batch): 0.008876\n",
            "\t partial train loss (single batch): 0.006400\n",
            "\n",
            " EPOCH 94/300 \t train loss 0.007638114504516125 \t val loss 0.0077893552370369434\n",
            "\t partial train loss (single batch): 0.008788\n",
            "\t partial train loss (single batch): 0.006340\n",
            "\n",
            " EPOCH 95/300 \t train loss 0.007564139552414417 \t val loss 0.007710681762546301\n",
            "\t partial train loss (single batch): 0.008690\n",
            "\t partial train loss (single batch): 0.006282\n",
            "\n",
            " EPOCH 96/300 \t train loss 0.007486007176339626 \t val loss 0.0076578580774366856\n",
            "\t partial train loss (single batch): 0.008603\n",
            "\t partial train loss (single batch): 0.006218\n",
            "\n",
            " EPOCH 97/300 \t train loss 0.007410393096506596 \t val loss 0.007588304113596678\n",
            "\t partial train loss (single batch): 0.008518\n",
            "\t partial train loss (single batch): 0.006164\n",
            "\n",
            " EPOCH 98/300 \t train loss 0.007340670563280582 \t val loss 0.007533114869147539\n",
            "\t partial train loss (single batch): 0.008426\n",
            "\t partial train loss (single batch): 0.006106\n",
            "\n",
            " EPOCH 99/300 \t train loss 0.00726553238928318 \t val loss 0.007461287546902895\n",
            "\t partial train loss (single batch): 0.008346\n",
            "\t partial train loss (single batch): 0.006050\n",
            "\n",
            " EPOCH 100/300 \t train loss 0.007197961211204529 \t val loss 0.007417688611894846\n",
            "\t partial train loss (single batch): 0.008256\n",
            "\t partial train loss (single batch): 0.005994\n",
            "\n",
            " EPOCH 101/300 \t train loss 0.0071250069886446 \t val loss 0.00736355222761631\n",
            "\t partial train loss (single batch): 0.008186\n",
            "\t partial train loss (single batch): 0.005941\n",
            "\n",
            " EPOCH 102/300 \t train loss 0.007063502911478281 \t val loss 0.00729385344311595\n",
            "\t partial train loss (single batch): 0.008092\n",
            "\t partial train loss (single batch): 0.005903\n",
            "\n",
            " EPOCH 103/300 \t train loss 0.006997562944889069 \t val loss 0.007270431146025658\n",
            "\t partial train loss (single batch): 0.008037\n",
            "\t partial train loss (single batch): 0.005840\n",
            "\n",
            " EPOCH 104/300 \t train loss 0.006938387639820576 \t val loss 0.007211461663246155\n",
            "\t partial train loss (single batch): 0.007959\n",
            "\t partial train loss (single batch): 0.005815\n",
            "\n",
            " EPOCH 105/300 \t train loss 0.0068868612870574 \t val loss 0.00718260183930397\n",
            "\t partial train loss (single batch): 0.007906\n",
            "\t partial train loss (single batch): 0.005760\n",
            "\n",
            " EPOCH 106/300 \t train loss 0.006832859478890896 \t val loss 0.00713138235732913\n",
            "\t partial train loss (single batch): 0.007831\n",
            "\t partial train loss (single batch): 0.005752\n",
            "\n",
            " EPOCH 107/300 \t train loss 0.006791654508560896 \t val loss 0.007107376586645842\n",
            "\t partial train loss (single batch): 0.007791\n",
            "\t partial train loss (single batch): 0.005694\n",
            "\n",
            " EPOCH 108/300 \t train loss 0.006742328405380249 \t val loss 0.007081170100718737\n",
            "\t partial train loss (single batch): 0.007734\n",
            "\t partial train loss (single batch): 0.005693\n",
            "\n",
            " EPOCH 109/300 \t train loss 0.0067136250436306 \t val loss 0.007045153062790632\n",
            "\t partial train loss (single batch): 0.007696\n",
            "\t partial train loss (single batch): 0.005633\n",
            "\n",
            " EPOCH 110/300 \t train loss 0.006664562039077282 \t val loss 0.007057386916130781\n",
            "\t partial train loss (single batch): 0.007653\n",
            "\t partial train loss (single batch): 0.005638\n",
            "\n",
            " EPOCH 111/300 \t train loss 0.006645575165748596 \t val loss 0.007013299036771059\n",
            "\t partial train loss (single batch): 0.007617\n",
            "\t partial train loss (single batch): 0.005581\n",
            "\n",
            " EPOCH 112/300 \t train loss 0.006598612293601036 \t val loss 0.007042118348181248\n",
            "\t partial train loss (single batch): 0.007581\n",
            "\t partial train loss (single batch): 0.005647\n",
            "\n",
            " EPOCH 113/300 \t train loss 0.00661396374925971 \t val loss 0.006962880492210388\n",
            "\t partial train loss (single batch): 0.007535\n",
            "\t partial train loss (single batch): 0.005713\n",
            "\n",
            " EPOCH 114/300 \t train loss 0.006623868830502033 \t val loss 0.0069346120581030846\n",
            "\t partial train loss (single batch): 0.007400\n",
            "\t partial train loss (single batch): 0.005898\n",
            "\n",
            " EPOCH 115/300 \t train loss 0.006648888811469078 \t val loss 0.006762583274394274\n",
            "\t partial train loss (single batch): 0.007247\n",
            "\t partial train loss (single batch): 0.005903\n",
            "\n",
            " EPOCH 116/300 \t train loss 0.0065748365595936775 \t val loss 0.006699404213577509\n",
            "\t partial train loss (single batch): 0.007176\n",
            "\t partial train loss (single batch): 0.005657\n",
            "\n",
            " EPOCH 117/300 \t train loss 0.006416365038603544 \t val loss 0.006885648239403963\n",
            "\t partial train loss (single batch): 0.007235\n",
            "\t partial train loss (single batch): 0.005334\n",
            "\n",
            " EPOCH 118/300 \t train loss 0.006284309085458517 \t val loss 0.006711777299642563\n",
            "\t partial train loss (single batch): 0.007206\n",
            "\t partial train loss (single batch): 0.005296\n",
            "\n",
            " EPOCH 119/300 \t train loss 0.006251058541238308 \t val loss 0.006719097960740328\n",
            "\t partial train loss (single batch): 0.007041\n",
            "\t partial train loss (single batch): 0.005390\n",
            "\n",
            " EPOCH 120/300 \t train loss 0.006215590052306652 \t val loss 0.006555917207151651\n",
            "\t partial train loss (single batch): 0.006928\n",
            "\t partial train loss (single batch): 0.005328\n",
            "\n",
            " EPOCH 121/300 \t train loss 0.0061281053349375725 \t val loss 0.006509801838546991\n",
            "\t partial train loss (single batch): 0.006899\n",
            "\t partial train loss (single batch): 0.005185\n",
            "\n",
            " EPOCH 122/300 \t train loss 0.00604183180257678 \t val loss 0.006556991953402758\n",
            "\t partial train loss (single batch): 0.006879\n",
            "\t partial train loss (single batch): 0.005114\n",
            "\n",
            " EPOCH 123/300 \t train loss 0.005996611900627613 \t val loss 0.006391745060682297\n",
            "\t partial train loss (single batch): 0.006768\n",
            "\t partial train loss (single batch): 0.005140\n",
            "\n",
            " EPOCH 124/300 \t train loss 0.00595388887450099 \t val loss 0.006394980475306511\n",
            "\t partial train loss (single batch): 0.006683\n",
            "\t partial train loss (single batch): 0.005076\n",
            "\n",
            " EPOCH 125/300 \t train loss 0.005879271775484085 \t val loss 0.006373988930135965\n",
            "\t partial train loss (single batch): 0.006673\n",
            "\t partial train loss (single batch): 0.004990\n",
            "\n",
            " EPOCH 126/300 \t train loss 0.005831312853842974 \t val loss 0.0063061825931072235\n",
            "\t partial train loss (single batch): 0.006610\n",
            "\t partial train loss (single batch): 0.004961\n",
            "\n",
            " EPOCH 127/300 \t train loss 0.00578570831567049 \t val loss 0.006280665285885334\n",
            "\t partial train loss (single batch): 0.006554\n",
            "\t partial train loss (single batch): 0.004942\n",
            "\n",
            " EPOCH 128/300 \t train loss 0.005748031660914421 \t val loss 0.006206716876477003\n",
            "\t partial train loss (single batch): 0.006481\n",
            "\t partial train loss (single batch): 0.004924\n",
            "\n",
            " EPOCH 129/300 \t train loss 0.005702625028789043 \t val loss 0.006165292579680681\n",
            "\t partial train loss (single batch): 0.006435\n",
            "\t partial train loss (single batch): 0.004862\n",
            "\n",
            " EPOCH 130/300 \t train loss 0.005648743361234665 \t val loss 0.006163736805319786\n",
            "\t partial train loss (single batch): 0.006402\n",
            "\t partial train loss (single batch): 0.004840\n",
            "\n",
            " EPOCH 131/300 \t train loss 0.005621382035315037 \t val loss 0.0060925655998289585\n",
            "\t partial train loss (single batch): 0.006334\n",
            "\t partial train loss (single batch): 0.004812\n",
            "\n",
            " EPOCH 132/300 \t train loss 0.005572859663516283 \t val loss 0.0060753910802304745\n",
            "\t partial train loss (single batch): 0.006278\n",
            "\t partial train loss (single batch): 0.004795\n",
            "\n",
            " EPOCH 133/300 \t train loss 0.005536714568734169 \t val loss 0.006037407089024782\n",
            "\t partial train loss (single batch): 0.006239\n",
            "\t partial train loss (single batch): 0.004747\n",
            "\n",
            " EPOCH 134/300 \t train loss 0.005492959637194872 \t val loss 0.006015153136104345\n",
            "\t partial train loss (single batch): 0.006192\n",
            "\t partial train loss (single batch): 0.004717\n",
            "\n",
            " EPOCH 135/300 \t train loss 0.005454519297927618 \t val loss 0.005967628210783005\n",
            "\t partial train loss (single batch): 0.006152\n",
            "\t partial train loss (single batch): 0.004698\n",
            "\n",
            " EPOCH 136/300 \t train loss 0.00542503222823143 \t val loss 0.005934693850576878\n",
            "\t partial train loss (single batch): 0.006090\n",
            "\t partial train loss (single batch): 0.004685\n",
            "\n",
            " EPOCH 137/300 \t train loss 0.0053871506825089455 \t val loss 0.005924316588789225\n",
            "\t partial train loss (single batch): 0.006065\n",
            "\t partial train loss (single batch): 0.004650\n",
            "\n",
            " EPOCH 138/300 \t train loss 0.005357327871024609 \t val loss 0.0058988542295992374\n",
            "\t partial train loss (single batch): 0.006048\n",
            "\t partial train loss (single batch): 0.004624\n",
            "\n",
            " EPOCH 139/300 \t train loss 0.0053359889425337315 \t val loss 0.00592230586335063\n",
            "\t partial train loss (single batch): 0.006030\n",
            "\t partial train loss (single batch): 0.004604\n",
            "\n",
            " EPOCH 140/300 \t train loss 0.005316892173141241 \t val loss 0.005873858463019133\n",
            "\t partial train loss (single batch): 0.005988\n",
            "\t partial train loss (single batch): 0.004640\n",
            "\n",
            " EPOCH 141/300 \t train loss 0.005313766188919544 \t val loss 0.005818741861730814\n",
            "\t partial train loss (single batch): 0.005914\n",
            "\t partial train loss (single batch): 0.004620\n",
            "\n",
            " EPOCH 142/300 \t train loss 0.005266871768981218 \t val loss 0.005785226821899414\n",
            "\t partial train loss (single batch): 0.005863\n",
            "\t partial train loss (single batch): 0.004585\n",
            "\n",
            " EPOCH 143/300 \t train loss 0.005223933141678572 \t val loss 0.005773497279733419\n",
            "\t partial train loss (single batch): 0.005859\n",
            "\t partial train loss (single batch): 0.004507\n",
            "\n",
            " EPOCH 144/300 \t train loss 0.005183090455830097 \t val loss 0.005766831338405609\n",
            "\t partial train loss (single batch): 0.005822\n",
            "\t partial train loss (single batch): 0.004503\n",
            "\n",
            " EPOCH 145/300 \t train loss 0.00516265956684947 \t val loss 0.005704546347260475\n",
            "\t partial train loss (single batch): 0.005765\n",
            "\t partial train loss (single batch): 0.004515\n",
            "\n",
            " EPOCH 146/300 \t train loss 0.00513977138325572 \t val loss 0.005684688221663237\n",
            "\t partial train loss (single batch): 0.005695\n",
            "\t partial train loss (single batch): 0.004503\n",
            "\n",
            " EPOCH 147/300 \t train loss 0.00509926863014698 \t val loss 0.005659118294715881\n",
            "\t partial train loss (single batch): 0.005684\n",
            "\t partial train loss (single batch): 0.004453\n",
            "\n",
            " EPOCH 148/300 \t train loss 0.005068250931799412 \t val loss 0.005664673633873463\n",
            "\t partial train loss (single batch): 0.005672\n",
            "\t partial train loss (single batch): 0.004449\n",
            "\n",
            " EPOCH 149/300 \t train loss 0.005060525611042976 \t val loss 0.00561925582587719\n",
            "\t partial train loss (single batch): 0.005623\n",
            "\t partial train loss (single batch): 0.004421\n",
            "\n",
            " EPOCH 150/300 \t train loss 0.0050222985446453094 \t val loss 0.0055772229097783566\n",
            "\t partial train loss (single batch): 0.005546\n",
            "\t partial train loss (single batch): 0.004460\n",
            "\n",
            " EPOCH 151/300 \t train loss 0.005002847872674465 \t val loss 0.0055327033624053\n",
            "\t partial train loss (single batch): 0.005493\n",
            "\t partial train loss (single batch): 0.004423\n",
            "\n",
            " EPOCH 152/300 \t train loss 0.004958042874932289 \t val loss 0.0054951319471001625\n",
            "\t partial train loss (single batch): 0.005438\n",
            "\t partial train loss (single batch): 0.004420\n",
            "\n",
            " EPOCH 153/300 \t train loss 0.004928812850266695 \t val loss 0.005481199361383915\n",
            "\t partial train loss (single batch): 0.005417\n",
            "\t partial train loss (single batch): 0.004343\n",
            "\n",
            " EPOCH 154/300 \t train loss 0.004880378022789955 \t val loss 0.0054381005465984344\n",
            "\t partial train loss (single batch): 0.005367\n",
            "\t partial train loss (single batch): 0.004320\n",
            "\n",
            " EPOCH 155/300 \t train loss 0.004843560047447681 \t val loss 0.005422141402959824\n",
            "\t partial train loss (single batch): 0.005332\n",
            "\t partial train loss (single batch): 0.004242\n",
            "\n",
            " EPOCH 156/300 \t train loss 0.004787350073456764 \t val loss 0.005401709582656622\n",
            "\t partial train loss (single batch): 0.005306\n",
            "\t partial train loss (single batch): 0.004222\n",
            "\n",
            " EPOCH 157/300 \t train loss 0.004764050245285034 \t val loss 0.005383326206356287\n",
            "\t partial train loss (single batch): 0.005270\n",
            "\t partial train loss (single batch): 0.004162\n",
            "\n",
            " EPOCH 158/300 \t train loss 0.004715523682534695 \t val loss 0.005391139071434736\n",
            "\t partial train loss (single batch): 0.005255\n",
            "\t partial train loss (single batch): 0.004147\n",
            "\n",
            " EPOCH 159/300 \t train loss 0.004700595512986183 \t val loss 0.005334960762411356\n",
            "\t partial train loss (single batch): 0.005214\n",
            "\t partial train loss (single batch): 0.004126\n",
            "\n",
            " EPOCH 160/300 \t train loss 0.004670027643442154 \t val loss 0.005360293202102184\n",
            "\t partial train loss (single batch): 0.005201\n",
            "\t partial train loss (single batch): 0.004105\n",
            "\n",
            " EPOCH 161/300 \t train loss 0.004652627743780613 \t val loss 0.005343390163034201\n",
            "\t partial train loss (single batch): 0.005180\n",
            "\t partial train loss (single batch): 0.004093\n",
            "\n",
            " EPOCH 162/300 \t train loss 0.004636653698980808 \t val loss 0.005322424694895744\n",
            "\t partial train loss (single batch): 0.005180\n",
            "\t partial train loss (single batch): 0.004060\n",
            "\n",
            " EPOCH 163/300 \t train loss 0.004619943909347057 \t val loss 0.005329122766852379\n",
            "\t partial train loss (single batch): 0.005145\n",
            "\t partial train loss (single batch): 0.004074\n",
            "\n",
            " EPOCH 164/300 \t train loss 0.004609454423189163 \t val loss 0.005266431253403425\n",
            "\t partial train loss (single batch): 0.005076\n",
            "\t partial train loss (single batch): 0.004080\n",
            "\n",
            " EPOCH 165/300 \t train loss 0.004577950574457645 \t val loss 0.005285143386572599\n",
            "\t partial train loss (single batch): 0.005087\n",
            "\t partial train loss (single batch): 0.004045\n",
            "\n",
            " EPOCH 166/300 \t train loss 0.0045659542083740234 \t val loss 0.0052817766554653645\n",
            "\t partial train loss (single batch): 0.005060\n",
            "\t partial train loss (single batch): 0.004036\n",
            "\n",
            " EPOCH 167/300 \t train loss 0.004547876305878162 \t val loss 0.00524758780375123\n",
            "\t partial train loss (single batch): 0.005077\n",
            "\t partial train loss (single batch): 0.004046\n",
            "\n",
            " EPOCH 168/300 \t train loss 0.004561545327305794 \t val loss 0.005286460742354393\n",
            "\t partial train loss (single batch): 0.005039\n",
            "\t partial train loss (single batch): 0.004073\n",
            "\n",
            " EPOCH 169/300 \t train loss 0.004555840976536274 \t val loss 0.005193592514842749\n",
            "\t partial train loss (single batch): 0.004974\n",
            "\t partial train loss (single batch): 0.004097\n",
            "\n",
            " EPOCH 170/300 \t train loss 0.0045355018228292465 \t val loss 0.005163191817700863\n",
            "\t partial train loss (single batch): 0.004933\n",
            "\t partial train loss (single batch): 0.004031\n",
            "\n",
            " EPOCH 171/300 \t train loss 0.0044820732437074184 \t val loss 0.0052291094325482845\n",
            "\t partial train loss (single batch): 0.004952\n",
            "\t partial train loss (single batch): 0.004008\n",
            "\n",
            " EPOCH 172/300 \t train loss 0.004479730501770973 \t val loss 0.005176588427275419\n",
            "\t partial train loss (single batch): 0.004909\n",
            "\t partial train loss (single batch): 0.003982\n",
            "\n",
            " EPOCH 173/300 \t train loss 0.004445334896445274 \t val loss 0.005242859944701195\n",
            "\t partial train loss (single batch): 0.004900\n",
            "\t partial train loss (single batch): 0.003964\n",
            "\n",
            " EPOCH 174/300 \t train loss 0.00443183071911335 \t val loss 0.0050682201981544495\n",
            "\t partial train loss (single batch): 0.004815\n",
            "\t partial train loss (single batch): 0.003964\n",
            "\n",
            " EPOCH 175/300 \t train loss 0.004389302805066109 \t val loss 0.0051075792871415615\n",
            "\t partial train loss (single batch): 0.004806\n",
            "\t partial train loss (single batch): 0.003896\n",
            "\n",
            " EPOCH 176/300 \t train loss 0.0043510389514267445 \t val loss 0.005052241962403059\n",
            "\t partial train loss (single batch): 0.004807\n",
            "\t partial train loss (single batch): 0.003854\n",
            "\n",
            " EPOCH 177/300 \t train loss 0.004330787342041731 \t val loss 0.0050516873598098755\n",
            "\t partial train loss (single batch): 0.004751\n",
            "\t partial train loss (single batch): 0.003846\n",
            "\n",
            " EPOCH 178/300 \t train loss 0.004298162180930376 \t val loss 0.004969487432390451\n",
            "\t partial train loss (single batch): 0.004687\n",
            "\t partial train loss (single batch): 0.003834\n",
            "\n",
            " EPOCH 179/300 \t train loss 0.004260610789060593 \t val loss 0.0049947951920330524\n",
            "\t partial train loss (single batch): 0.004650\n",
            "\t partial train loss (single batch): 0.003780\n",
            "\n",
            " EPOCH 180/300 \t train loss 0.004215081222355366 \t val loss 0.004947264213114977\n",
            "\t partial train loss (single batch): 0.004609\n",
            "\t partial train loss (single batch): 0.003799\n",
            "\n",
            " EPOCH 181/300 \t train loss 0.004204391036182642 \t val loss 0.004915663041174412\n",
            "\t partial train loss (single batch): 0.004578\n",
            "\t partial train loss (single batch): 0.003724\n",
            "\n",
            " EPOCH 182/300 \t train loss 0.004150822293013334 \t val loss 0.004959963262081146\n",
            "\t partial train loss (single batch): 0.004570\n",
            "\t partial train loss (single batch): 0.003737\n",
            "\n",
            " EPOCH 183/300 \t train loss 0.004153370391577482 \t val loss 0.004874669015407562\n",
            "\t partial train loss (single batch): 0.004520\n",
            "\t partial train loss (single batch): 0.003712\n",
            "\n",
            " EPOCH 184/300 \t train loss 0.004115777090191841 \t val loss 0.004885893780738115\n",
            "\t partial train loss (single batch): 0.004494\n",
            "\t partial train loss (single batch): 0.003677\n",
            "\n",
            " EPOCH 185/300 \t train loss 0.00408569723367691 \t val loss 0.00484615471214056\n",
            "\t partial train loss (single batch): 0.004473\n",
            "\t partial train loss (single batch): 0.003662\n",
            "\n",
            " EPOCH 186/300 \t train loss 0.004067474510520697 \t val loss 0.004831239115446806\n",
            "\t partial train loss (single batch): 0.004431\n",
            "\t partial train loss (single batch): 0.003640\n",
            "\n",
            " EPOCH 187/300 \t train loss 0.0040353890508413315 \t val loss 0.004820253700017929\n",
            "\t partial train loss (single batch): 0.004419\n",
            "\t partial train loss (single batch): 0.003638\n",
            "\n",
            " EPOCH 188/300 \t train loss 0.004028817173093557 \t val loss 0.004782048985362053\n",
            "\t partial train loss (single batch): 0.004372\n",
            "\t partial train loss (single batch): 0.003627\n",
            "\n",
            " EPOCH 189/300 \t train loss 0.003999794367700815 \t val loss 0.004794951528310776\n",
            "\t partial train loss (single batch): 0.004370\n",
            "\t partial train loss (single batch): 0.003569\n",
            "\n",
            " EPOCH 190/300 \t train loss 0.0039695329032838345 \t val loss 0.004784387070685625\n",
            "\t partial train loss (single batch): 0.004362\n",
            "\t partial train loss (single batch): 0.003573\n",
            "\n",
            " EPOCH 191/300 \t train loss 0.003967409022152424 \t val loss 0.004746988415718079\n",
            "\t partial train loss (single batch): 0.004319\n",
            "\t partial train loss (single batch): 0.003564\n",
            "\n",
            " EPOCH 192/300 \t train loss 0.003941201139241457 \t val loss 0.004752038512378931\n",
            "\t partial train loss (single batch): 0.004295\n",
            "\t partial train loss (single batch): 0.003549\n",
            "\n",
            " EPOCH 193/300 \t train loss 0.003922066651284695 \t val loss 0.004725346807390451\n",
            "\t partial train loss (single batch): 0.004285\n",
            "\t partial train loss (single batch): 0.003524\n",
            "\n",
            " EPOCH 194/300 \t train loss 0.003904391545802355 \t val loss 0.004752022679895163\n",
            "\t partial train loss (single batch): 0.004261\n",
            "\t partial train loss (single batch): 0.003511\n",
            "\n",
            " EPOCH 195/300 \t train loss 0.0038858233019709587 \t val loss 0.004718135576695204\n",
            "\t partial train loss (single batch): 0.004257\n",
            "\t partial train loss (single batch): 0.003533\n",
            "\n",
            " EPOCH 196/300 \t train loss 0.0038948929868638515 \t val loss 0.0046863192692399025\n",
            "\t partial train loss (single batch): 0.004215\n",
            "\t partial train loss (single batch): 0.003511\n",
            "\n",
            " EPOCH 197/300 \t train loss 0.0038633965887129307 \t val loss 0.004724432714283466\n",
            "\t partial train loss (single batch): 0.004233\n",
            "\t partial train loss (single batch): 0.003481\n",
            "\n",
            " EPOCH 198/300 \t train loss 0.0038569364696741104 \t val loss 0.004691534209996462\n",
            "\t partial train loss (single batch): 0.004218\n",
            "\t partial train loss (single batch): 0.003482\n",
            "\n",
            " EPOCH 199/300 \t train loss 0.003849675878882408 \t val loss 0.004716317169368267\n",
            "\t partial train loss (single batch): 0.004212\n",
            "\t partial train loss (single batch): 0.003475\n",
            "\n",
            " EPOCH 200/300 \t train loss 0.0038434986490756273 \t val loss 0.004711672198027372\n",
            "\t partial train loss (single batch): 0.004190\n",
            "\t partial train loss (single batch): 0.003489\n",
            "\n",
            " EPOCH 201/300 \t train loss 0.0038390622939914465 \t val loss 0.004649645183235407\n",
            "\t partial train loss (single batch): 0.004197\n",
            "\t partial train loss (single batch): 0.003431\n",
            "\n",
            " EPOCH 202/300 \t train loss 0.0038141701370477676 \t val loss 0.00474055390805006\n",
            "\t partial train loss (single batch): 0.004197\n",
            "\t partial train loss (single batch): 0.003421\n",
            "\n",
            " EPOCH 203/300 \t train loss 0.0038086017593741417 \t val loss 0.0046255821362137794\n",
            "\t partial train loss (single batch): 0.004147\n",
            "\t partial train loss (single batch): 0.003475\n",
            "\n",
            " EPOCH 204/300 \t train loss 0.0038109058514237404 \t val loss 0.004629682283848524\n",
            "\t partial train loss (single batch): 0.004058\n",
            "\t partial train loss (single batch): 0.003449\n",
            "\n",
            " EPOCH 205/300 \t train loss 0.0037533671129494905 \t val loss 0.004610727075487375\n",
            "\t partial train loss (single batch): 0.004074\n",
            "\t partial train loss (single batch): 0.003441\n",
            "\n",
            " EPOCH 206/300 \t train loss 0.0037576695904135704 \t val loss 0.004644426517188549\n",
            "\t partial train loss (single batch): 0.004082\n",
            "\t partial train loss (single batch): 0.003369\n",
            "\n",
            " EPOCH 207/300 \t train loss 0.003725188784301281 \t val loss 0.00468462985008955\n",
            "\t partial train loss (single batch): 0.004106\n",
            "\t partial train loss (single batch): 0.003319\n",
            "\n",
            " EPOCH 208/300 \t train loss 0.003712367732077837 \t val loss 0.004609608557075262\n",
            "\t partial train loss (single batch): 0.004060\n",
            "\t partial train loss (single batch): 0.003353\n",
            "\n",
            " EPOCH 209/300 \t train loss 0.0037066317163407803 \t val loss 0.004568396136164665\n",
            "\t partial train loss (single batch): 0.003996\n",
            "\t partial train loss (single batch): 0.003322\n",
            "\n",
            " EPOCH 210/300 \t train loss 0.0036589752417057753 \t val loss 0.004532537888735533\n",
            "\t partial train loss (single batch): 0.003975\n",
            "\t partial train loss (single batch): 0.003342\n",
            "\n",
            " EPOCH 211/300 \t train loss 0.0036584832705557346 \t val loss 0.0044881426729261875\n",
            "\t partial train loss (single batch): 0.003923\n",
            "\t partial train loss (single batch): 0.003330\n",
            "\n",
            " EPOCH 212/300 \t train loss 0.0036265510134398937 \t val loss 0.0045084585435688496\n",
            "\t partial train loss (single batch): 0.003914\n",
            "\t partial train loss (single batch): 0.003332\n",
            "\n",
            " EPOCH 213/300 \t train loss 0.003623252734541893 \t val loss 0.004468924831598997\n",
            "\t partial train loss (single batch): 0.003884\n",
            "\t partial train loss (single batch): 0.003321\n",
            "\n",
            " EPOCH 214/300 \t train loss 0.0036024886649101973 \t val loss 0.004472154192626476\n",
            "\t partial train loss (single batch): 0.003854\n",
            "\t partial train loss (single batch): 0.003286\n",
            "\n",
            " EPOCH 215/300 \t train loss 0.003569744760170579 \t val loss 0.004461510106921196\n",
            "\t partial train loss (single batch): 0.003870\n",
            "\t partial train loss (single batch): 0.003254\n",
            "\n",
            " EPOCH 216/300 \t train loss 0.0035619912669062614 \t val loss 0.00445018382743001\n",
            "\t partial train loss (single batch): 0.003825\n",
            "\t partial train loss (single batch): 0.003256\n",
            "\n",
            " EPOCH 217/300 \t train loss 0.003540679346770048 \t val loss 0.004424992948770523\n",
            "\t partial train loss (single batch): 0.003826\n",
            "\t partial train loss (single batch): 0.003247\n",
            "\n",
            " EPOCH 218/300 \t train loss 0.0035365838557481766 \t val loss 0.004409470595419407\n",
            "\t partial train loss (single batch): 0.003778\n",
            "\t partial train loss (single batch): 0.003266\n",
            "\n",
            " EPOCH 219/300 \t train loss 0.003521950216963887 \t val loss 0.0044034733437001705\n",
            "\t partial train loss (single batch): 0.003779\n",
            "\t partial train loss (single batch): 0.003232\n",
            "\n",
            " EPOCH 220/300 \t train loss 0.0035056807100772858 \t val loss 0.004392815753817558\n",
            "\t partial train loss (single batch): 0.003770\n",
            "\t partial train loss (single batch): 0.003235\n",
            "\n",
            " EPOCH 221/300 \t train loss 0.003502429695799947 \t val loss 0.004415187053382397\n",
            "\t partial train loss (single batch): 0.003749\n",
            "\t partial train loss (single batch): 0.003214\n",
            "\n",
            " EPOCH 222/300 \t train loss 0.0034817829728126526 \t val loss 0.004354421049356461\n",
            "\t partial train loss (single batch): 0.003732\n",
            "\t partial train loss (single batch): 0.003207\n",
            "\n",
            " EPOCH 223/300 \t train loss 0.0034697060473263264 \t val loss 0.0043562548235058784\n",
            "\t partial train loss (single batch): 0.003683\n",
            "\t partial train loss (single batch): 0.003188\n",
            "\n",
            " EPOCH 224/300 \t train loss 0.00343539216555655 \t val loss 0.004304076079279184\n",
            "\t partial train loss (single batch): 0.003661\n",
            "\t partial train loss (single batch): 0.003167\n",
            "\n",
            " EPOCH 225/300 \t train loss 0.0034141119103878736 \t val loss 0.004292329773306847\n",
            "\t partial train loss (single batch): 0.003637\n",
            "\t partial train loss (single batch): 0.003147\n",
            "\n",
            " EPOCH 226/300 \t train loss 0.00339204678311944 \t val loss 0.004279933404177427\n",
            "\t partial train loss (single batch): 0.003612\n",
            "\t partial train loss (single batch): 0.003120\n",
            "\n",
            " EPOCH 227/300 \t train loss 0.0033656859304755926 \t val loss 0.004275146871805191\n",
            "\t partial train loss (single batch): 0.003601\n",
            "\t partial train loss (single batch): 0.003093\n",
            "\n",
            " EPOCH 228/300 \t train loss 0.0033467356115579605 \t val loss 0.004253110848367214\n",
            "\t partial train loss (single batch): 0.003579\n",
            "\t partial train loss (single batch): 0.003072\n",
            "\n",
            " EPOCH 229/300 \t train loss 0.0033252588473260403 \t val loss 0.004262847360223532\n",
            "\t partial train loss (single batch): 0.003567\n",
            "\t partial train loss (single batch): 0.003049\n",
            "\n",
            " EPOCH 230/300 \t train loss 0.0033080256544053555 \t val loss 0.0042417398653924465\n",
            "\t partial train loss (single batch): 0.003553\n",
            "\t partial train loss (single batch): 0.003038\n",
            "\n",
            " EPOCH 231/300 \t train loss 0.0032956297509372234 \t val loss 0.004236096050590277\n",
            "\t partial train loss (single batch): 0.003531\n",
            "\t partial train loss (single batch): 0.003026\n",
            "\n",
            " EPOCH 232/300 \t train loss 0.0032786407973617315 \t val loss 0.0042492724023759365\n",
            "\t partial train loss (single batch): 0.003522\n",
            "\t partial train loss (single batch): 0.003015\n",
            "\n",
            " EPOCH 233/300 \t train loss 0.003268254455178976 \t val loss 0.004204214084893465\n",
            "\t partial train loss (single batch): 0.003507\n",
            "\t partial train loss (single batch): 0.003018\n",
            "\n",
            " EPOCH 234/300 \t train loss 0.0032623223960399628 \t val loss 0.004189811181277037\n",
            "\t partial train loss (single batch): 0.003480\n",
            "\t partial train loss (single batch): 0.003011\n",
            "\n",
            " EPOCH 235/300 \t train loss 0.0032455557957291603 \t val loss 0.004236034583300352\n",
            "\t partial train loss (single batch): 0.003469\n",
            "\t partial train loss (single batch): 0.002984\n",
            "\n",
            " EPOCH 236/300 \t train loss 0.003226536326110363 \t val loss 0.004179386887699366\n",
            "\t partial train loss (single batch): 0.003467\n",
            "\t partial train loss (single batch): 0.003005\n",
            "\n",
            " EPOCH 237/300 \t train loss 0.0032361741177737713 \t val loss 0.004173662979155779\n",
            "\t partial train loss (single batch): 0.003446\n",
            "\t partial train loss (single batch): 0.002981\n",
            "\n",
            " EPOCH 238/300 \t train loss 0.003213736228644848 \t val loss 0.0042287008836865425\n",
            "\t partial train loss (single batch): 0.003469\n",
            "\t partial train loss (single batch): 0.002995\n",
            "\n",
            " EPOCH 239/300 \t train loss 0.0032323431223630905 \t val loss 0.004164272919297218\n",
            "\t partial train loss (single batch): 0.003414\n",
            "\t partial train loss (single batch): 0.003024\n",
            "\n",
            " EPOCH 240/300 \t train loss 0.0032194245140999556 \t val loss 0.0042104278691112995\n",
            "\t partial train loss (single batch): 0.003488\n",
            "\t partial train loss (single batch): 0.002964\n",
            "\n",
            " EPOCH 241/300 \t train loss 0.0032258322462439537 \t val loss 0.004242240451276302\n",
            "\t partial train loss (single batch): 0.003467\n",
            "\t partial train loss (single batch): 0.003040\n",
            "\n",
            " EPOCH 242/300 \t train loss 0.003253187285736203 \t val loss 0.004163511097431183\n",
            "\t partial train loss (single batch): 0.003401\n",
            "\t partial train loss (single batch): 0.003105\n",
            "\n",
            " EPOCH 243/300 \t train loss 0.003253151196986437 \t val loss 0.004257754888385534\n",
            "\t partial train loss (single batch): 0.003489\n",
            "\t partial train loss (single batch): 0.003016\n",
            "\n",
            " EPOCH 244/300 \t train loss 0.0032525756396353245 \t val loss 0.004359796643257141\n",
            "\t partial train loss (single batch): 0.003540\n",
            "\t partial train loss (single batch): 0.003006\n",
            "\n",
            " EPOCH 245/300 \t train loss 0.0032729213126003742 \t val loss 0.004239477217197418\n",
            "\t partial train loss (single batch): 0.003533\n",
            "\t partial train loss (single batch): 0.003194\n",
            "\n",
            " EPOCH 246/300 \t train loss 0.003363911062479019 \t val loss 0.004200842697173357\n",
            "\t partial train loss (single batch): 0.003429\n",
            "\t partial train loss (single batch): 0.003175\n",
            "\n",
            " EPOCH 247/300 \t train loss 0.0033023031428456306 \t val loss 0.0042232912965118885\n",
            "\t partial train loss (single batch): 0.003491\n",
            "\t partial train loss (single batch): 0.003119\n",
            "\n",
            " EPOCH 248/300 \t train loss 0.003305101301521063 \t val loss 0.004270242992788553\n",
            "\t partial train loss (single batch): 0.003546\n",
            "\t partial train loss (single batch): 0.003018\n",
            "\n",
            " EPOCH 249/300 \t train loss 0.003282062476500869 \t val loss 0.004357292316854\n",
            "\t partial train loss (single batch): 0.003534\n",
            "\t partial train loss (single batch): 0.002977\n",
            "\n",
            " EPOCH 250/300 \t train loss 0.003255345392972231 \t val loss 0.004313504323363304\n",
            "\t partial train loss (single batch): 0.003541\n",
            "\t partial train loss (single batch): 0.002977\n",
            "\n",
            " EPOCH 251/300 \t train loss 0.0032590175978839397 \t val loss 0.004301035776734352\n",
            "\t partial train loss (single batch): 0.003491\n",
            "\t partial train loss (single batch): 0.002975\n",
            "\n",
            " EPOCH 252/300 \t train loss 0.0032333359122276306 \t val loss 0.004250138532370329\n",
            "\t partial train loss (single batch): 0.003476\n",
            "\t partial train loss (single batch): 0.003035\n",
            "\n",
            " EPOCH 253/300 \t train loss 0.0032555479556322098 \t val loss 0.004211327061057091\n",
            "\t partial train loss (single batch): 0.003424\n",
            "\t partial train loss (single batch): 0.003052\n",
            "\n",
            " EPOCH 254/300 \t train loss 0.0032381881028413773 \t val loss 0.004225266166031361\n",
            "\t partial train loss (single batch): 0.003417\n",
            "\t partial train loss (single batch): 0.003119\n",
            "\n",
            " EPOCH 255/300 \t train loss 0.003267870284616947 \t val loss 0.004159901756793261\n",
            "\t partial train loss (single batch): 0.003374\n",
            "\t partial train loss (single batch): 0.003164\n",
            "\n",
            " EPOCH 256/300 \t train loss 0.0032686428166925907 \t val loss 0.004270503763109446\n",
            "\t partial train loss (single batch): 0.003443\n",
            "\t partial train loss (single batch): 0.003131\n",
            "\n",
            " EPOCH 257/300 \t train loss 0.003286834107711911 \t val loss 0.004296517930924892\n",
            "\t partial train loss (single batch): 0.003466\n",
            "\t partial train loss (single batch): 0.003060\n",
            "\n",
            " EPOCH 258/300 \t train loss 0.0032630448695272207 \t val loss 0.00437479792162776\n",
            "\t partial train loss (single batch): 0.003576\n",
            "\t partial train loss (single batch): 0.002984\n",
            "\n",
            " EPOCH 259/300 \t train loss 0.0032802773639559746 \t val loss 0.004276562947779894\n",
            "\t partial train loss (single batch): 0.003423\n",
            "\t partial train loss (single batch): 0.002962\n",
            "\n",
            " EPOCH 260/300 \t train loss 0.003192213363945484 \t val loss 0.004333658143877983\n",
            "\t partial train loss (single batch): 0.003407\n",
            "\t partial train loss (single batch): 0.003019\n",
            "\n",
            " EPOCH 261/300 \t train loss 0.0032130354084074497 \t val loss 0.0040339878760278225\n",
            "\t partial train loss (single batch): 0.003239\n",
            "\t partial train loss (single batch): 0.003093\n",
            "\n",
            " EPOCH 262/300 \t train loss 0.003165984759107232 \t val loss 0.004012273624539375\n",
            "\t partial train loss (single batch): 0.003207\n",
            "\t partial train loss (single batch): 0.003024\n",
            "\n",
            " EPOCH 263/300 \t train loss 0.0031155473552644253 \t val loss 0.004044126253575087\n",
            "\t partial train loss (single batch): 0.003225\n",
            "\t partial train loss (single batch): 0.002972\n",
            "\n",
            " EPOCH 264/300 \t train loss 0.003098757704719901 \t val loss 0.0040466743521392345\n",
            "\t partial train loss (single batch): 0.003216\n",
            "\t partial train loss (single batch): 0.002896\n",
            "\n",
            " EPOCH 265/300 \t train loss 0.0030559743754565716 \t val loss 0.004106849431991577\n",
            "\t partial train loss (single batch): 0.003261\n",
            "\t partial train loss (single batch): 0.002864\n",
            "\n",
            " EPOCH 266/300 \t train loss 0.003062489442527294 \t val loss 0.004048062488436699\n",
            "\t partial train loss (single batch): 0.003175\n",
            "\t partial train loss (single batch): 0.002838\n",
            "\n",
            " EPOCH 267/300 \t train loss 0.0030064922757446766 \t val loss 0.00397886848077178\n",
            "\t partial train loss (single batch): 0.003165\n",
            "\t partial train loss (single batch): 0.002806\n",
            "\n",
            " EPOCH 268/300 \t train loss 0.0029857391491532326 \t val loss 0.003941221162676811\n",
            "\t partial train loss (single batch): 0.003129\n",
            "\t partial train loss (single batch): 0.002808\n",
            "\n",
            " EPOCH 269/300 \t train loss 0.0029689278453588486 \t val loss 0.003962296526879072\n",
            "\t partial train loss (single batch): 0.003107\n",
            "\t partial train loss (single batch): 0.002786\n",
            "\n",
            " EPOCH 270/300 \t train loss 0.0029466901905834675 \t val loss 0.003902968717738986\n",
            "\t partial train loss (single batch): 0.003099\n",
            "\t partial train loss (single batch): 0.002777\n",
            "\n",
            " EPOCH 271/300 \t train loss 0.0029379120096564293 \t val loss 0.003924231044948101\n",
            "\t partial train loss (single batch): 0.003075\n",
            "\t partial train loss (single batch): 0.002753\n",
            "\n",
            " EPOCH 272/300 \t train loss 0.0029140962287783623 \t val loss 0.0038935309275984764\n",
            "\t partial train loss (single batch): 0.003069\n",
            "\t partial train loss (single batch): 0.002746\n",
            "\n",
            " EPOCH 273/300 \t train loss 0.0029076605569571257 \t val loss 0.0038866891991347075\n",
            "\t partial train loss (single batch): 0.003048\n",
            "\t partial train loss (single batch): 0.002749\n",
            "\n",
            " EPOCH 274/300 \t train loss 0.0028989738784730434 \t val loss 0.003867041552439332\n",
            "\t partial train loss (single batch): 0.003031\n",
            "\t partial train loss (single batch): 0.002742\n",
            "\n",
            " EPOCH 275/300 \t train loss 0.0028863600455224514 \t val loss 0.003870086744427681\n",
            "\t partial train loss (single batch): 0.003023\n",
            "\t partial train loss (single batch): 0.002737\n",
            "\n",
            " EPOCH 276/300 \t train loss 0.0028801606968045235 \t val loss 0.003881619544699788\n",
            "\t partial train loss (single batch): 0.003021\n",
            "\t partial train loss (single batch): 0.002721\n",
            "\n",
            " EPOCH 277/300 \t train loss 0.002871054457500577 \t val loss 0.0038780199829488993\n",
            "\t partial train loss (single batch): 0.003032\n",
            "\t partial train loss (single batch): 0.002722\n",
            "\n",
            " EPOCH 278/300 \t train loss 0.0028772265650331974 \t val loss 0.003893746994435787\n",
            "\t partial train loss (single batch): 0.003021\n",
            "\t partial train loss (single batch): 0.002728\n",
            "\n",
            " EPOCH 279/300 \t train loss 0.0028746756725013256 \t val loss 0.0038757568690925837\n",
            "\t partial train loss (single batch): 0.003020\n",
            "\t partial train loss (single batch): 0.002725\n",
            "\n",
            " EPOCH 280/300 \t train loss 0.002872698474675417 \t val loss 0.0038497576024383307\n",
            "\t partial train loss (single batch): 0.002995\n",
            "\t partial train loss (single batch): 0.002736\n",
            "\n",
            " EPOCH 281/300 \t train loss 0.002865840680897236 \t val loss 0.003854580456390977\n",
            "\t partial train loss (single batch): 0.002994\n",
            "\t partial train loss (single batch): 0.002715\n",
            "\n",
            " EPOCH 282/300 \t train loss 0.0028541996143758297 \t val loss 0.00382076483219862\n",
            "\t partial train loss (single batch): 0.002972\n",
            "\t partial train loss (single batch): 0.002720\n",
            "\n",
            " EPOCH 283/300 \t train loss 0.0028457925654947758 \t val loss 0.003837746102362871\n",
            "\t partial train loss (single batch): 0.002969\n",
            "\t partial train loss (single batch): 0.002693\n",
            "\n",
            " EPOCH 284/300 \t train loss 0.0028310713823884726 \t val loss 0.0038054457399994135\n",
            "\t partial train loss (single batch): 0.002956\n",
            "\t partial train loss (single batch): 0.002712\n",
            "\n",
            " EPOCH 285/300 \t train loss 0.0028340001590549946 \t val loss 0.0038218761328607798\n",
            "\t partial train loss (single batch): 0.002942\n",
            "\t partial train loss (single batch): 0.002704\n",
            "\n",
            " EPOCH 286/300 \t train loss 0.002823272719979286 \t val loss 0.0037886935751885176\n",
            "\t partial train loss (single batch): 0.002931\n",
            "\t partial train loss (single batch): 0.002724\n",
            "\n",
            " EPOCH 287/300 \t train loss 0.0028274620417505503 \t val loss 0.0038041225634515285\n",
            "\t partial train loss (single batch): 0.002917\n",
            "\t partial train loss (single batch): 0.002704\n",
            "\n",
            " EPOCH 288/300 \t train loss 0.002810175996273756 \t val loss 0.003772192867472768\n",
            "\t partial train loss (single batch): 0.002912\n",
            "\t partial train loss (single batch): 0.002695\n",
            "\n",
            " EPOCH 289/300 \t train loss 0.0028038821183145046 \t val loss 0.003798543708398938\n",
            "\t partial train loss (single batch): 0.002905\n",
            "\t partial train loss (single batch): 0.002673\n",
            "\n",
            " EPOCH 290/300 \t train loss 0.0027891150675714016 \t val loss 0.0037685278803110123\n",
            "\t partial train loss (single batch): 0.002895\n",
            "\t partial train loss (single batch): 0.002665\n",
            "\n",
            " EPOCH 291/300 \t train loss 0.0027800165116786957 \t val loss 0.003800584701821208\n",
            "\t partial train loss (single batch): 0.002891\n",
            "\t partial train loss (single batch): 0.002656\n",
            "\n",
            " EPOCH 292/300 \t train loss 0.0027735955081880093 \t val loss 0.003763404441997409\n",
            "\t partial train loss (single batch): 0.002880\n",
            "\t partial train loss (single batch): 0.002654\n",
            "\n",
            " EPOCH 293/300 \t train loss 0.0027673901058733463 \t val loss 0.0038025015965104103\n",
            "\t partial train loss (single batch): 0.002883\n",
            "\t partial train loss (single batch): 0.002648\n",
            "\n",
            " EPOCH 294/300 \t train loss 0.0027652070857584476 \t val loss 0.0037531759589910507\n",
            "\t partial train loss (single batch): 0.002871\n",
            "\t partial train loss (single batch): 0.002647\n",
            "\n",
            " EPOCH 295/300 \t train loss 0.002758966526016593 \t val loss 0.0037851964589208364\n",
            "\t partial train loss (single batch): 0.002871\n",
            "\t partial train loss (single batch): 0.002636\n",
            "\n",
            " EPOCH 296/300 \t train loss 0.002753778826445341 \t val loss 0.0037533242721110582\n",
            "\t partial train loss (single batch): 0.002860\n",
            "\t partial train loss (single batch): 0.002635\n",
            "\n",
            " EPOCH 297/300 \t train loss 0.002747836522758007 \t val loss 0.003771302755922079\n",
            "\t partial train loss (single batch): 0.002863\n",
            "\t partial train loss (single batch): 0.002621\n",
            "\n",
            " EPOCH 298/300 \t train loss 0.0027419920079410076 \t val loss 0.003747172188013792\n",
            "\t partial train loss (single batch): 0.002855\n",
            "\t partial train loss (single batch): 0.002611\n",
            "\n",
            " EPOCH 299/300 \t train loss 0.0027327979914844036 \t val loss 0.0037551759742200375\n",
            "\t partial train loss (single batch): 0.002847\n",
            "\t partial train loss (single batch): 0.002598\n",
            "\n",
            " EPOCH 300/300 \t train loss 0.0027224859222769737 \t val loss 0.0037287112791091204\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x324 with 20 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dp = Data_Preparation()\n"
      ],
      "metadata": {
        "id": "ZYOp6tTWjueG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dp.resize_image(folder_path='/content/drive/MyDrive/data-download',resized_images_folder_path= '/content/drive/MyDrive/data-download/resized-images/',pixel_length_cm = 250)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXeRcNWqkWi-",
        "outputId": "e3ac8ec5-0769-460c-e52e-cea3d0fbe5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:00<00:00, 86.99it/s]\n",
            "1it [03:54, 234.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_026031_2295_UNFILTERED_COLOR.IMG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r2it [05:23, 148.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_030429_2160_UNFILTERED_COLOR.IMG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r3it [09:14, 186.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_025952_2250_UNFILTERED_COLOR.IMG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r4it [09:54, 128.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_028953_1830_UNFILTERED_COLOR.IMG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r5it [13:41, 164.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_032100_1645_UNFILTERED_COLOR.IMG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r6it [14:28, 124.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_020887_1670_COLOR.IMG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r7it [16:36, 125.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_027173_1635_UNFILTERED_COLOR.IMG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r8it [18:04, 113.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_020115_0985_COLOR.IMG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r9it [19:42, 108.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_046350_2310_UNFILTERED_COLOR.IMG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r10it [21:04, 100.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_040829_1590_UNFILTERED_COLOR.IMG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r11it [22:10, 89.92s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_045314_1985_UNFILTERED_COLOR.IMG\n",
            "Hello\n",
            "/content/drive/MyDrive/data-download/ESP_046329_1725_UNFILTERED_COLOR.IMG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r12it [22:23, 66.51s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PoYZDG0d9AeT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}